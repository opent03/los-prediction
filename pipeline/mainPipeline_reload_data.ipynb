{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afe3a83e-8d72-4c42-bcda-368ebc89e9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs01/home/chloexq/.cache/pypoetry/virtualenvs/time-series-library-UC7OlRP4-py3.8/lib/python3.8/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "\n",
    "module_path='preprocessing/day_intervals_preproc'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "module_path='utils'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "module_path='preprocessing/hosp_module_preproc'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "module_path='model'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "#print(sys.path)\n",
    "root_dir = os.path.dirname(os.path.abspath('UserInterface.ipynb'))\n",
    "data_dir = '/datasets/MIMIC-IV/physionet.org/files'\n",
    "import day_intervals_cohort\n",
    "from day_intervals_cohort import *\n",
    "\n",
    "import day_intervals_cohort_v2\n",
    "from day_intervals_cohort_v2 import *\n",
    "\n",
    "import data_generation_icu\n",
    "\n",
    "import data_generation\n",
    "import evaluation\n",
    "\n",
    "import feature_selection_hosp\n",
    "from feature_selection_hosp import *\n",
    "\n",
    "# import train\n",
    "# from train import *\n",
    "\n",
    "\n",
    "import ml_models\n",
    "from ml_models import *\n",
    "\n",
    "import dl_train\n",
    "from dl_train import *\n",
    "\n",
    "import tokenization\n",
    "from tokenization import *\n",
    "\n",
    "\n",
    "import behrt_train\n",
    "from behrt_train import *\n",
    "\n",
    "import feature_selection_icu\n",
    "from feature_selection_icu import *\n",
    "import fairness\n",
    "import callibrate_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "nutritional-chicago",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "importlib.reload(day_intervals_cohort)\n",
    "import day_intervals_cohort\n",
    "from day_intervals_cohort import *\n",
    "\n",
    "importlib.reload(day_intervals_cohort_v2)\n",
    "import day_intervals_cohort_v2\n",
    "from day_intervals_cohort_v2 import *\n",
    "\n",
    "importlib.reload(data_generation_icu)\n",
    "import data_generation_icu\n",
    "importlib.reload(data_generation)\n",
    "import data_generation\n",
    "\n",
    "importlib.reload(feature_selection_hosp)\n",
    "import feature_selection_hosp\n",
    "from feature_selection_hosp import *\n",
    "\n",
    "importlib.reload(feature_selection_icu)\n",
    "import feature_selection_icu\n",
    "from feature_selection_icu import *\n",
    "\n",
    "importlib.reload(tokenization)\n",
    "import tokenization\n",
    "from tokenization import *\n",
    "\n",
    "importlib.reload(ml_models)\n",
    "import ml_models\n",
    "from ml_models import *\n",
    "\n",
    "importlib.reload(dl_train)\n",
    "import dl_train\n",
    "from dl_train import *\n",
    "\n",
    "importlib.reload(behrt_train)\n",
    "import behrt_train\n",
    "from behrt_train import *\n",
    "\n",
    "importlib.reload(fairness)\n",
    "import fairness\n",
    "\n",
    "importlib.reload(callibrate_output)\n",
    "import callibrate_output\n",
    "\n",
    "importlib.reload(evaluation)\n",
    "import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ce5ae64-a123-4f1a-85d9-65fcc9053a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"set random seed.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-demand",
   "metadata": {},
   "source": [
    "# Welcome to your MIMIC-IV Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-factor",
   "metadata": {},
   "source": [
    "## 10. Running BEHRT\n",
    "Below we integrate the implementation of BEHRT in our pipeline.\n",
    "We perform pre-procesing needed to run BEHRT model. https://github.com/deepmedicine/BEHRT\n",
    "\n",
    "Few things to note before running BEHRT -\n",
    "- The numerical values are binned into quantiles.\n",
    "- BEHRT has recommended maximum number of events per sample to be 512. \n",
    "    So feature selection is important so that number of events per sample does not exceed 512.\n",
    "- The model is quite computationally heavy so it requires a GPU.\n",
    "\n",
    "The output files for BEHRT will be saved in ./data/behrt/ folder\n",
    "\n",
    "**Please run below cell to to pre-process and run BEHRT on the selected cohort**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc8e396d-de14-49eb-8f6e-8589da45997d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'importlib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mimportlib\u001b[49m\u001b[38;5;241m.\u001b[39mreload(tokenization)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtokenization\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtokenization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'importlib' is not defined"
     ]
    }
   ],
   "source": [
    "importlib.reload(tokenization)\n",
    "import tokenization\n",
    "from tokenization import *\n",
    "\n",
    "import behrt_train\n",
    "from behrt_train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e47fe094-ec05-48c9-a0c1-5264e93c9aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_icu = True\n",
    "diag_flag = True\n",
    "proc_flag = False\n",
    "out_flag = False\n",
    "chart_flag = True\n",
    "med_flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aggressive-break",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING READING FILES.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:07<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED READING FILES. \n",
      "\n",
      "STARTING TOKENIZATION.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:06<00:00,  8.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED TOKENIZATION. \n",
      "\n",
      "FINAL COHORT STATISTICS: \n",
      "10 Positive samples.\n",
      "40 Negative samples.\n",
      "\n",
      "28 Female samples.\n",
      "22 Male samples.\n",
      "\n",
      "8 BLACK/AFRICAN AMERICAN samples.\n",
      "34 WHITE samples.\n",
      "3 UNKNOWN samples.\n",
      "1 BLACK/CAPE VERDEAN samples.\n",
      "1 WHITE - BRAZILIAN samples.\n",
      "1 BLACK/AFRICAN samples.\n",
      "1 HISPANIC OR LATINO samples.\n",
      "1 OTHER samples.\n",
      "0 UNABLE TO OBTAIN samples.\n",
      "0 HISPANIC/LATINO - SALVADORAN samples.\n",
      "0 WHITE - OTHER EUROPEAN samples.\n",
      "0 BLACK/CARIBBEAN ISLAND samples.\n",
      "0 ASIAN samples.\n",
      "0 HISPANIC/LATINO - DOMINICAN samples.\n",
      "0 HISPANIC/LATINO - PUERTO RICAN samples.\n",
      "0 WHITE - RUSSIAN samples.\n",
      "0 ASIAN - KOREAN samples.\n",
      "0 ASIAN - CHINESE samples.\n",
      "0 PATIENT DECLINED TO ANSWER samples.\n",
      "0 WHITE - EASTERN EUROPEAN samples.\n",
      "0 ASIAN - ASIAN INDIAN samples.\n",
      "0 ASIAN - SOUTH EAST ASIAN samples.\n",
      "0 NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER samples.\n",
      "0 AMERICAN INDIAN/ALASKA NATIVE samples.\n",
      "0 HISPANIC/LATINO - CENTRAL AMERICAN samples.\n",
      "0 PORTUGUESE samples.\n",
      "0 HISPANIC/LATINO - GUATEMALAN samples.\n",
      "0 HISPANIC/LATINO - HONDURAN samples.\n",
      "0 HISPANIC/LATINO - CUBAN samples.\n",
      "0 SOUTH AMERICAN samples.\n",
      "0 MULTIPLE RACE/ETHNICITY samples.\n",
      "0 HISPANIC/LATINO - COLUMBIAN samples.\n",
      "0 HISPANIC/LATINO - MEXICAN samples.\n",
      "\n",
      "\n",
      "30 Medicare samples.\n",
      "14 Other samples.\n",
      "6 Medicaid samples.\n",
      "Epoch n0\n",
      "0.6719639897346497\n",
      "Finished train\n",
      "TOTAL LOSS 0.7022700309753418\n",
      "TRAIN 0.6544548471768697\t0.45267462730407715 secs\n",
      "\n",
      "TRAIN loss_cls 1.9633645415306091 loss_dab 2.0826125741004944\n",
      "\n",
      "EVAL 0.7022700309753418\t0.02708721160888672 secs\n",
      "\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "Epoch n1\n",
      "0.6221106648445129\n",
      "Finished train\n",
      "TOTAL LOSS 0.7010704874992371\n",
      "TRAIN 0.6253582040468851\t0.4328937530517578 secs\n",
      "\n",
      "TRAIN loss_cls 1.8760746121406555 loss_dab 2.0911989212036133\n",
      "\n",
      "EVAL 0.7010704874992371\t0.026439428329467773 secs\n",
      "\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "Epoch n2\n",
      "0.5670081377029419\n",
      "Finished train\n",
      "TOTAL LOSS 0.7002684473991394\n",
      "TRAIN 0.6330129504203796\t0.42904138565063477 secs\n",
      "\n",
      "TRAIN loss_cls 1.899038851261139 loss_dab 2.0912686586380005\n",
      "\n",
      "EVAL 0.7002684473991394\t0.02684783935546875 secs\n",
      "\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "Epoch n3\n",
      "0.5681015253067017\n",
      "Finished train\n",
      "TOTAL LOSS 0.703093409538269\n",
      "TRAIN 0.5312538544336954\t0.4294703006744385 secs\n",
      "\n",
      "TRAIN loss_cls 1.5937615633010864 loss_dab 2.106478750705719\n",
      "\n",
      "EVAL 0.703093409538269\t0.026726961135864258 secs\n",
      "\n",
      "Epoch n4\n",
      "0.5209200978279114\n",
      "Finished train\n",
      "TOTAL LOSS 0.7089427709579468\n",
      "TRAIN 0.5542389551798502\t0.42159366607666016 secs\n",
      "\n",
      "TRAIN loss_cls 1.6627168655395508 loss_dab 2.0855724215507507\n",
      "\n",
      "EVAL 0.7089427709579468\t0.025711774826049805 secs\n",
      "\n",
      "Epoch n5\n",
      "0.5666687488555908\n",
      "Finished train\n",
      "TOTAL LOSS 0.7172359228134155\n",
      "TRAIN 0.48089341322580975\t0.4224236011505127 secs\n",
      "\n",
      "TRAIN loss_cls 1.4426802396774292 loss_dab 2.092789590358734\n",
      "\n",
      "EVAL 0.7172359228134155\t0.02624821662902832 secs\n",
      "\n",
      "Epoch n6\n",
      "0.5206040740013123\n",
      "Finished train\n",
      "TOTAL LOSS 0.7274687886238098\n",
      "TRAIN 0.5240117112795512\t0.4248075485229492 secs\n",
      "\n",
      "TRAIN loss_cls 1.5720351338386536 loss_dab 2.0997314453125\n",
      "\n",
      "EVAL 0.7274687886238098\t0.027054309844970703 secs\n",
      "\n",
      "Epoch n7\n",
      "0.4544001519680023\n",
      "Finished train\n",
      "TOTAL LOSS 0.738419234752655\n",
      "TRAIN 0.5190286934375763\t0.41875720024108887 secs\n",
      "\n",
      "TRAIN loss_cls 1.5570860803127289 loss_dab 2.107061743736267\n",
      "\n",
      "EVAL 0.738419234752655\t0.026038408279418945 secs\n",
      "\n",
      "Epoch n8\n",
      "0.4199766516685486\n",
      "Finished train\n",
      "TOTAL LOSS 0.7487646341323853\n",
      "TRAIN 0.5107580025990804\t0.4250321388244629 secs\n",
      "\n",
      "TRAIN loss_cls 1.5322740077972412 loss_dab 2.1079280972480774\n",
      "\n",
      "EVAL 0.7487646341323853\t0.026218891143798828 secs\n",
      "\n",
      "Epoch n9\n",
      "0.5787158012390137\n",
      "Finished train\n",
      "TOTAL LOSS 0.7576245665550232\n",
      "TRAIN 0.4078032871087392\t0.42917346954345703 secs\n",
      "\n",
      "TRAIN loss_cls 1.2234098613262177 loss_dab 2.10247403383255\n",
      "\n",
      "EVAL 0.7576245665550232\t0.027122974395751953 secs\n",
      "\n",
      "Epoch n10\n",
      "0.4555480480194092\n",
      "Finished train\n",
      "TOTAL LOSS 0.7674434781074524\n",
      "TRAIN 0.5161085923512777\t0.42801856994628906 secs\n",
      "\n",
      "TRAIN loss_cls 1.548325777053833 loss_dab 2.1085994243621826\n",
      "\n",
      "EVAL 0.7674434781074524\t0.02703690528869629 secs\n",
      "\n",
      "Epoch n11\n",
      "0.34735456109046936\n",
      "Finished train\n",
      "TOTAL LOSS 0.7757226824760437\n",
      "TRAIN 0.4027845760186513\t0.4245882034301758 secs\n",
      "\n",
      "TRAIN loss_cls 1.208353728055954 loss_dab 2.129872441291809\n",
      "\n",
      "EVAL 0.7757226824760437\t0.028261661529541016 secs\n",
      "\n",
      "Epoch n12\n",
      "0.38893717527389526\n",
      "Finished train\n",
      "TOTAL LOSS 0.7845579385757446\n",
      "TRAIN 0.38472286860148114\t0.44201087951660156 secs\n",
      "\n",
      "TRAIN loss_cls 1.1541686058044434 loss_dab 2.1232489347457886\n",
      "\n",
      "EVAL 0.7845579385757446\t0.027630329132080078 secs\n",
      "\n",
      "Epoch n13\n",
      "0.4549762010574341\n",
      "Finished train\n",
      "TOTAL LOSS 0.7944783568382263\n",
      "TRAIN 0.3872178892294566\t0.43265271186828613 secs\n",
      "\n",
      "TRAIN loss_cls 1.1616536676883698 loss_dab 2.114919662475586\n",
      "\n",
      "EVAL 0.7944783568382263\t0.027968168258666992 secs\n",
      "\n",
      "Epoch n14\n",
      "0.5034996271133423\n",
      "Finished train\n",
      "TOTAL LOSS 0.8056555986404419\n",
      "TRAIN 0.48240553339322406\t0.432025671005249 secs\n",
      "\n",
      "TRAIN loss_cls 1.4472166001796722 loss_dab 2.119621157646179\n",
      "\n",
      "EVAL 0.8056555986404419\t0.028240442276000977 secs\n",
      "\n",
      "Epoch n15\n",
      "0.36970388889312744\n",
      "Finished train\n",
      "TOTAL LOSS 0.8159114718437195\n",
      "TRAIN 0.4565556049346924\t0.4337131977081299 secs\n",
      "\n",
      "TRAIN loss_cls 1.3696668148040771 loss_dab 2.1029672622680664\n",
      "\n",
      "EVAL 0.8159114718437195\t0.029121875762939453 secs\n",
      "\n",
      "Epoch n16\n",
      "0.514543354511261\n",
      "Finished train\n",
      "TOTAL LOSS 0.825178325176239\n",
      "TRAIN 0.37419772148132324\t0.4384958744049072 secs\n",
      "\n",
      "TRAIN loss_cls 1.1225931644439697 loss_dab 2.1099693179130554\n",
      "\n",
      "EVAL 0.825178325176239\t0.028140783309936523 secs\n",
      "\n",
      "Epoch n17\n",
      "0.4452112019062042\n",
      "Finished train\n",
      "TOTAL LOSS 0.835743248462677\n",
      "TRAIN 0.35807598133881885\t0.4245936870574951 secs\n",
      "\n",
      "TRAIN loss_cls 1.0742279440164566 loss_dab 2.11227947473526\n",
      "\n",
      "EVAL 0.835743248462677\t0.027393579483032227 secs\n",
      "\n",
      "Epoch n18\n",
      "0.4186984598636627\n",
      "Finished train\n",
      "TOTAL LOSS 0.8457488417625427\n",
      "TRAIN 0.4425651629765828\t0.4237501621246338 secs\n",
      "\n",
      "TRAIN loss_cls 1.3276954889297485 loss_dab 2.1139559149742126\n",
      "\n",
      "EVAL 0.8457488417625427\t0.02591562271118164 secs\n",
      "\n",
      "Epoch n19\n",
      "0.3629482388496399\n",
      "Finished train\n",
      "TOTAL LOSS 0.8544484972953796\n",
      "TRAIN 0.34123412768046063\t0.4251875877380371 secs\n",
      "\n",
      "TRAIN loss_cls 1.0237023830413818 loss_dab 2.108042597770691\n",
      "\n",
      "EVAL 0.8544484972953796\t0.027615070343017578 secs\n",
      "\n",
      "Epoch n20\n",
      "0.361838161945343\n",
      "Finished train\n",
      "TOTAL LOSS 0.8636720776557922\n",
      "TRAIN 0.4150211115678151\t0.43668627738952637 secs\n",
      "\n",
      "TRAIN loss_cls 1.2450633347034454 loss_dab 2.1185376048088074\n",
      "\n",
      "EVAL 0.8636720776557922\t0.02801060676574707 secs\n",
      "\n",
      "Epoch n21\n",
      "0.39228853583335876\n",
      "Finished train\n",
      "TOTAL LOSS 0.8715410232543945\n",
      "TRAIN 0.4276858369509379\t0.43358635902404785 secs\n",
      "\n",
      "TRAIN loss_cls 1.2830575108528137 loss_dab 2.1304489970207214\n",
      "\n",
      "EVAL 0.8715410232543945\t0.02750873565673828 secs\n",
      "\n",
      "Epoch n22\n",
      "0.32985132932662964\n",
      "Finished train\n",
      "TOTAL LOSS 0.8765097856521606\n",
      "TRAIN 0.31345658500989276\t0.42984962463378906 secs\n",
      "\n",
      "TRAIN loss_cls 0.9403697550296783 loss_dab 2.1240352392196655\n",
      "\n",
      "EVAL 0.8765097856521606\t0.026137113571166992 secs\n",
      "\n",
      "Epoch n23\n",
      "0.35829421877861023\n",
      "Finished train\n",
      "TOTAL LOSS 0.8840568661689758\n",
      "TRAIN 0.3333410322666168\t0.42409229278564453 secs\n",
      "\n",
      "TRAIN loss_cls 1.0000230967998505 loss_dab 2.1510936617851257\n",
      "\n",
      "EVAL 0.8840568661689758\t0.02565908432006836 secs\n",
      "\n",
      "Epoch n24\n",
      "0.3588073253631592\n",
      "Finished train\n",
      "TOTAL LOSS 0.8907747268676758\n",
      "TRAIN 0.29948628942171734\t0.42307353019714355 secs\n",
      "\n",
      "TRAIN loss_cls 0.898458868265152 loss_dab 2.1095146536827087\n",
      "\n",
      "EVAL 0.8907747268676758\t0.025594711303710938 secs\n",
      "\n",
      "Epoch n25\n",
      "0.28356635570526123\n",
      "Finished train\n",
      "TOTAL LOSS 0.8991627097129822\n",
      "TRAIN 0.309809277455012\t0.42519450187683105 secs\n",
      "\n",
      "TRAIN loss_cls 0.929427832365036 loss_dab 2.108245551586151\n",
      "\n",
      "EVAL 0.8991627097129822\t0.026362180709838867 secs\n",
      "\n",
      "Epoch n26\n",
      "0.449734091758728\n",
      "Finished train\n",
      "TOTAL LOSS 0.9079057574272156\n",
      "TRAIN 0.28930242856343585\t0.42482519149780273 secs\n",
      "\n",
      "TRAIN loss_cls 0.8679072856903076 loss_dab 2.108463227748871\n",
      "\n",
      "EVAL 0.9079057574272156\t0.02645134925842285 secs\n",
      "\n",
      "Epoch n27\n",
      "0.4235963523387909\n",
      "Finished train\n",
      "TOTAL LOSS 0.9194592833518982\n",
      "TRAIN 0.27835013965765637\t0.4345109462738037 secs\n",
      "\n",
      "TRAIN loss_cls 0.835050418972969 loss_dab 2.1243714094161987\n",
      "\n",
      "EVAL 0.9194592833518982\t0.025848865509033203 secs\n",
      "\n",
      "Epoch n28\n",
      "0.3844587802886963\n",
      "Finished train\n",
      "TOTAL LOSS 0.9321986436843872\n",
      "TRAIN 0.26639240980148315\t0.42731761932373047 secs\n",
      "\n",
      "TRAIN loss_cls 0.7991772294044495 loss_dab 2.1190396547317505\n",
      "\n",
      "EVAL 0.9321986436843872\t0.027938127517700195 secs\n",
      "\n",
      "Epoch n29\n",
      "0.26034659147262573\n",
      "Finished train\n",
      "TOTAL LOSS 0.9451214671134949\n",
      "TRAIN 0.2868754466374715\t0.4250178337097168 secs\n",
      "\n",
      "TRAIN loss_cls 0.8606263399124146 loss_dab 2.1232893466949463\n",
      "\n",
      "EVAL 0.9451214671134949\t0.02568531036376953 secs\n",
      "\n",
      "Epoch n30\n",
      "0.22467826306819916\n",
      "Finished train\n",
      "TOTAL LOSS 0.9580545425415039\n",
      "TRAIN 0.22768689692020416\t0.42637181282043457 secs\n",
      "\n",
      "TRAIN loss_cls 0.6830606907606125 loss_dab 2.1269680857658386\n",
      "\n",
      "EVAL 0.9580545425415039\t0.025655031204223633 secs\n",
      "\n",
      "Epoch n31\n",
      "0.22976914048194885\n",
      "Finished train\n",
      "TOTAL LOSS 0.9699870944023132\n",
      "TRAIN 0.21546890338261923\t0.42722654342651367 secs\n",
      "\n",
      "TRAIN loss_cls 0.6464067101478577 loss_dab 2.127482771873474\n",
      "\n",
      "EVAL 0.9699870944023132\t0.025849103927612305 secs\n",
      "\n",
      "Epoch n32\n",
      "0.2558484673500061\n",
      "Finished train\n",
      "TOTAL LOSS 0.97866290807724\n",
      "TRAIN 0.23084156215190887\t0.42243003845214844 secs\n",
      "\n",
      "TRAIN loss_cls 0.6925246864557266 loss_dab 2.128096044063568\n",
      "\n",
      "EVAL 0.97866290807724\t0.02730393409729004 secs\n",
      "\n",
      "Epoch n33\n",
      "0.1724286675453186\n",
      "Finished train\n",
      "TOTAL LOSS 0.9865037798881531\n",
      "TRAIN 0.18773851792017618\t0.4275667667388916 secs\n",
      "\n",
      "TRAIN loss_cls 0.5632155537605286 loss_dab 2.1215999126434326\n",
      "\n",
      "EVAL 0.9865037798881531\t0.027502775192260742 secs\n",
      "\n",
      "Epoch n34\n",
      "0.1787019670009613\n",
      "Finished train\n",
      "TOTAL LOSS 0.9980974197387695\n",
      "TRAIN 0.17159792532523474\t0.4263010025024414 secs\n",
      "\n",
      "TRAIN loss_cls 0.5147937759757042 loss_dab 2.1288777589797974\n",
      "\n",
      "EVAL 0.9980974197387695\t0.026701927185058594 secs\n",
      "\n",
      "Epoch n35\n",
      "0.2189025729894638\n",
      "Finished train\n",
      "TOTAL LOSS 1.008378505706787\n",
      "TRAIN 0.1629505455493927\t0.4357569217681885 secs\n",
      "\n",
      "TRAIN loss_cls 0.4888516366481781 loss_dab 2.1218932271003723\n",
      "\n",
      "EVAL 1.008378505706787\t0.03175950050354004 secs\n",
      "\n",
      "Epoch n36\n",
      "0.17656758427619934\n",
      "Finished train\n",
      "TOTAL LOSS 1.0219541788101196\n",
      "TRAIN 0.15623707075913748\t0.43287134170532227 secs\n",
      "\n",
      "TRAIN loss_cls 0.4687112122774124 loss_dab 2.131743371486664\n",
      "\n",
      "EVAL 1.0219541788101196\t0.027925491333007812 secs\n",
      "\n",
      "Epoch n37\n",
      "0.1402871012687683\n",
      "Finished train\n",
      "TOTAL LOSS 1.0415409803390503\n",
      "TRAIN 0.14415550728638968\t0.4358687400817871 secs\n",
      "\n",
      "TRAIN loss_cls 0.432466521859169 loss_dab 2.128653049468994\n",
      "\n",
      "EVAL 1.0415409803390503\t0.027880430221557617 secs\n",
      "\n",
      "Epoch n38\n",
      "0.1328851878643036\n",
      "Finished train\n",
      "TOTAL LOSS 1.0607527494430542\n",
      "TRAIN 0.13772442440191904\t0.4336848258972168 secs\n",
      "\n",
      "TRAIN loss_cls 0.41317327320575714 loss_dab 2.1302754878997803\n",
      "\n",
      "EVAL 1.0607527494430542\t0.027985572814941406 secs\n",
      "\n",
      "Epoch n39\n",
      "0.09231086820363998\n",
      "Finished train\n",
      "TOTAL LOSS 1.0634864568710327\n",
      "TRAIN 0.147129125893116\t0.4394381046295166 secs\n",
      "\n",
      "TRAIN loss_cls 0.441387377679348 loss_dab 2.1150839924812317\n",
      "\n",
      "EVAL 1.0634864568710327\t0.028013944625854492 secs\n",
      "\n",
      "Epoch n40\n",
      "0.1066003367304802\n",
      "Finished train\n",
      "TOTAL LOSS 1.0465484857559204\n",
      "TRAIN 0.11926689992348354\t0.43976879119873047 secs\n",
      "\n",
      "TRAIN loss_cls 0.3578006997704506 loss_dab 2.127759337425232\n",
      "\n",
      "EVAL 1.0465484857559204\t0.027954816818237305 secs\n",
      "\n",
      "Epoch n41\n",
      "0.0756751000881195\n",
      "Finished train\n",
      "TOTAL LOSS 1.0157198905944824\n",
      "TRAIN 0.0769211823741595\t0.4372105598449707 secs\n",
      "\n",
      "TRAIN loss_cls 0.23076354712247849 loss_dab 2.136567711830139\n",
      "\n",
      "EVAL 1.0157198905944824\t0.03189802169799805 secs\n",
      "\n",
      "Epoch n42\n",
      "0.07763861119747162\n",
      "Finished train\n",
      "TOTAL LOSS 0.9875124096870422\n",
      "TRAIN 0.07820934802293777\t0.4407083988189697 secs\n",
      "\n",
      "TRAIN loss_cls 0.23462804406881332 loss_dab 2.1393014788627625\n",
      "\n",
      "EVAL 0.9875124096870422\t0.029367923736572266 secs\n",
      "\n",
      "Epoch n43\n",
      "0.07066789269447327\n",
      "Finished train\n",
      "TOTAL LOSS 0.9720231890678406\n",
      "TRAIN 0.07313138494888942\t0.4325437545776367 secs\n",
      "\n",
      "TRAIN loss_cls 0.21939415484666824 loss_dab 2.1405381560325623\n",
      "\n",
      "EVAL 0.9720231890678406\t0.02569723129272461 secs\n",
      "\n",
      "Epoch n44\n",
      "0.07511644810438156\n",
      "Finished train\n",
      "TOTAL LOSS 0.9842084050178528\n",
      "TRAIN 0.06573451807101567\t0.42591166496276855 secs\n",
      "\n",
      "TRAIN loss_cls 0.19720355421304703 loss_dab 2.1318594813346863\n",
      "\n",
      "EVAL 0.9842084050178528\t0.027685165405273438 secs\n",
      "\n",
      "Epoch n45\n",
      "0.0650683045387268\n",
      "Finished train\n",
      "TOTAL LOSS 1.015399694442749\n",
      "TRAIN 0.05859881639480591\t0.4275827407836914 secs\n",
      "\n",
      "TRAIN loss_cls 0.17579644918441772 loss_dab 2.130585253238678\n",
      "\n",
      "EVAL 1.015399694442749\t0.026762723922729492 secs\n",
      "\n",
      "Epoch n46\n",
      "0.05442454293370247\n",
      "Finished train\n",
      "TOTAL LOSS 1.0563403367996216\n",
      "TRAIN 0.05999856318036715\t0.4299032688140869 secs\n",
      "\n",
      "TRAIN loss_cls 0.17999568954110146 loss_dab 2.1325971484184265\n",
      "\n",
      "EVAL 1.0563403367996216\t0.02817678451538086 secs\n",
      "\n",
      "Epoch n47\n",
      "0.055209722369909286\n",
      "Finished train\n",
      "TOTAL LOSS 1.103753685951233\n",
      "TRAIN 0.05205829069018364\t0.4265782833099365 secs\n",
      "\n",
      "TRAIN loss_cls 0.15617487207055092 loss_dab 2.1423234343528748\n",
      "\n",
      "EVAL 1.103753685951233\t0.02574443817138672 secs\n",
      "\n",
      "Epoch n48\n",
      "0.04916684702038765\n",
      "Finished train\n",
      "TOTAL LOSS 1.1474071741104126\n",
      "TRAIN 0.04741726815700531\t0.4354248046875 secs\n",
      "\n",
      "TRAIN loss_cls 0.14225180447101593 loss_dab 2.134762465953827\n",
      "\n",
      "EVAL 1.1474071741104126\t0.028473854064941406 secs\n",
      "\n",
      "Epoch n49\n",
      "0.0514938049018383\n",
      "Finished train\n",
      "TOTAL LOSS 1.1831473112106323\n",
      "TRAIN 0.04728567972779274\t0.4268519878387451 secs\n",
      "\n",
      "TRAIN loss_cls 0.14185703918337822 loss_dab 2.1323623657226562\n",
      "\n",
      "EVAL 1.1831473112106323\t0.028909921646118164 secs\n",
      "\n",
      "Loading succesfull\n",
      "TOTAL LOSS 0.5811478495597839\n",
      "tensor([[0.3617],\n",
      "        [0.3433],\n",
      "        [0.4292],\n",
      "        [0.3665],\n",
      "        [0.3502],\n",
      "        [0.3918],\n",
      "        [0.3677],\n",
      "        [0.3913],\n",
      "        [0.3952],\n",
      "        [0.3614]])\n",
      "tensor(0.3125)\n",
      "tensor(0.2250)\n",
      "tensor(0.)\n",
      "tensor(0.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<behrt_train.train_behrt at 0x7f33c8c55160>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if data_icu:\n",
    "    token=tokenization.BEHRT_models(data_icu,diag_flag,proc_flag,out_flag,chart_flag,med_flag,False)\n",
    "    tokenized_src, tokenized_age, tokenized_gender, tokenized_ethni, tokenized_ins, tokenized_labels, labs, meds, meds_labels, n_meds =token.tokenize()\n",
    "else:\n",
    "    token=tokenization.BEHRT_models(data_icu,diag_flag,proc_flag,False,False,med_flag,lab_flag)\n",
    "    tokenized_src, tokenized_age, tokenized_gender, tokenized_ethni, tokenized_ins, tokenized_labels=token.tokenize()\n",
    "    \n",
    "behrt_train.train_behrt(tokenized_src, tokenized_age, tokenized_gender, tokenized_ethni, tokenized_ins, tokenized_labels, labs, meds, meds_labels, n_meds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-objective",
   "metadata": {},
   "source": [
    "### EVALUATION AS STANDALONE MODULE\n",
    "Below cell shows an exaple of how evaluation module can be used as a standalone module.\n",
    "\n",
    "evaluation.Loss class can be instantiated and model output and ground truth can be passed to it to obtain results.\n",
    "\n",
    "In the example below we captured model output and ground truth in a file and used that file to read the data.\n",
    "\n",
    "In function definition ***loss(prob,truth,logits,False)***\n",
    "\n",
    "prob -> List of Output predicted probabilities of case being positive\n",
    "\n",
    "truth -> List of ground truth labels\n",
    "\n",
    "logits -> List of logits obtained from last fully connected layer before applying softmax.sigmoid function in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device='cuda:0'\n",
    "#device='cpu'\n",
    "loss=evaluation.Loss(device,acc=True,ppv=True,sensi=True,tnr=True,npv=True,auroc=True,aurocPlot=True,auprc=True,auprcPlot=True,callb=True,callbPlot=True)\n",
    "with open(\"./data/output/outputDict\", 'rb') as fp:\n",
    "    outputDict=pickle.load(fp)\n",
    "prob=list(outputDict['Prob'])\n",
    "truth=list(outputDict['Labels'])\n",
    "logits=list(outputDict['Logits'])\n",
    "#print(torch.tensor(prob))\n",
    "print(\"======= TESTING ========\")\n",
    "loss(prob,truth,logits,train=False,standalone=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-works",
   "metadata": {},
   "source": [
    "### 11. FAIRNESS EVALUATION\n",
    "In train and testing step we save output files in **./data/output/** folder.\n",
    "\n",
    "This file conatins list of demographic variables included in training and testing of the model.\n",
    "\n",
    "It also contains the ground truth labels and predicted probability for each sample.\n",
    "\n",
    "We use the above saved data to perform fairness evaluation of the results obtained from model testing.\n",
    "\n",
    "This module can be used as stand-alone module also.\n",
    "\n",
    "Please create a file that contains predicted probabilites form the last sigmoid layer in column named **Prob** and\n",
    "ground truth labels for each sample in column named **Labels**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-direction",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness.fairness_evaluation(inputFile='outputDict',outputFile='fairnessReport')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-distinction",
   "metadata": {},
   "source": [
    "### 12. MODEL CALLIBRATION\n",
    "\n",
    "Please run below cell if you want to callibrate predicted probabilites of the model on test data.\n",
    "It will use the output saved during the testing of the model.\n",
    "\n",
    "The file is saved in **./data/output/**.\n",
    "\n",
    "This module can be used as stand-alone module also.\n",
    "\n",
    "Please create a file that contain predicted logits form the last fully connected layer in column named **Logits** and <br>ground truth labels for each sample in a column named **Labels**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-flavor",
   "metadata": {},
   "outputs": [],
   "source": [
    "callibrate_output.callibrate(inputFile='outputDict',outputFile='callibratedResults')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-canyon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
