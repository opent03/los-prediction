{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "available-albany",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs01/home/chloexq/.cache/pypoetry/virtualenvs/time-series-library-UC7OlRP4-py3.8/lib/python3.8/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "\n",
    "module_path='preprocessing/day_intervals_preproc'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "module_path='utils'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "module_path='preprocessing/hosp_module_preproc'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "module_path='model'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "#print(sys.path)\n",
    "root_dir = os.path.dirname(os.path.abspath('UserInterface.ipynb'))\n",
    "data_dir = '/datasets/MIMIC-IV/physionet.org/files'\n",
    "import day_intervals_cohort\n",
    "from day_intervals_cohort import *\n",
    "\n",
    "import day_intervals_cohort_v2\n",
    "from day_intervals_cohort_v2 import *\n",
    "\n",
    "import data_generation_icu\n",
    "\n",
    "import data_generation\n",
    "import evaluation\n",
    "\n",
    "import feature_selection_hosp\n",
    "from feature_selection_hosp import *\n",
    "\n",
    "# import train\n",
    "# from train import *\n",
    "\n",
    "\n",
    "import ml_models\n",
    "from ml_models import *\n",
    "\n",
    "import dl_train\n",
    "from dl_train import *\n",
    "\n",
    "import tokenization\n",
    "from tokenization import *\n",
    "\n",
    "\n",
    "import behrt_train\n",
    "from behrt_train import *\n",
    "\n",
    "import feature_selection_icu\n",
    "from feature_selection_icu import *\n",
    "import fairness\n",
    "import callibrate_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "nutritional-chicago",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "importlib.reload(day_intervals_cohort)\n",
    "import day_intervals_cohort\n",
    "from day_intervals_cohort import *\n",
    "\n",
    "importlib.reload(day_intervals_cohort_v2)\n",
    "import day_intervals_cohort_v2\n",
    "from day_intervals_cohort_v2 import *\n",
    "\n",
    "importlib.reload(data_generation_icu)\n",
    "import data_generation_icu\n",
    "importlib.reload(data_generation)\n",
    "import data_generation\n",
    "\n",
    "importlib.reload(feature_selection_hosp)\n",
    "import feature_selection_hosp\n",
    "from feature_selection_hosp import *\n",
    "\n",
    "importlib.reload(feature_selection_icu)\n",
    "import feature_selection_icu\n",
    "from feature_selection_icu import *\n",
    "\n",
    "importlib.reload(tokenization)\n",
    "import tokenization\n",
    "from tokenization import *\n",
    "\n",
    "importlib.reload(ml_models)\n",
    "import ml_models\n",
    "from ml_models import *\n",
    "\n",
    "importlib.reload(dl_train)\n",
    "import dl_train\n",
    "from dl_train import *\n",
    "\n",
    "importlib.reload(behrt_train)\n",
    "import behrt_train\n",
    "from behrt_train import *\n",
    "\n",
    "importlib.reload(fairness)\n",
    "import fairness\n",
    "\n",
    "importlib.reload(callibrate_output)\n",
    "import callibrate_output\n",
    "\n",
    "importlib.reload(evaluation)\n",
    "import evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-demand",
   "metadata": {},
   "source": [
    "# Welcome to your MIMIC-IV Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-crown",
   "metadata": {},
   "source": [
    "This repository explains the steps to download and clean MIMIC-IV dataset for analysis.\n",
    "The repository is compatible with MIMIC-IV v1.0 and MIMIC-IV v2.0\n",
    "\n",
    "Please go to:\n",
    "- https://physionet.org/content/mimiciv/1.0/ for v1.0\n",
    "- https://physionet.org/content/mimiciv/2.0/ for v2.0\n",
    "\n",
    "Follow instructions to get access to MIMIC-IV dataset.\n",
    "\n",
    "Download the files using your terminal: \n",
    "- wget -r -N -c -np --user mehakg --ask-password https://physionet.org/files/mimiciv/1.0/ or\n",
    "- wget -r -N -c -np --user mehakg --ask-password https://physionet.org/files/mimiciv/2.0/\n",
    "        \n",
    "\n",
    "Save downloaded files in the parent directory of this github repo. \n",
    "\n",
    "The structure should look like below for v1.0-\n",
    "- mimiciv/1.0/core\n",
    "- mimiciv/1.0/hosp\n",
    "- mimiciv/1.0/icu\n",
    "\n",
    "The structure should look like below for v2.0-\n",
    "- mimiciv/2.0/hosp\n",
    "- mimiciv/2.0/icu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaf3028-7dbc-4185-bc64-a78b8a3277d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(dl_train)\n",
    "import dl_train\n",
    "from dl_train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c255bea5-3ca8-4659-85bd-430cf70d7c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Time-series LSTM'\n",
    "oversampling = True\n",
    "cv = 0\n",
    "data_icu = True\n",
    "diag_flag = True\n",
    "proc_flag = False\n",
    "out_flag = False\n",
    "chart_flag = True\n",
    "med_flag = True\n",
    "data_dir = '/datasets/MIMIC-IV'#'/scratch/ssd004/scratch/chloexq/data_demo/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-stewart",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if data_icu:\n",
    "    model=dl_train.DL_models(data_dir, data_icu,diag_flag,proc_flag,out_flag,chart_flag,med_flag,False,model_name,cv,oversampling,model_name='attn_icu_read',train=True)\n",
    "else:\n",
    "    model=dl_train.DL_models(data_dir, data_icu,diag_flag,proc_flag,False,False,med_flag,lab_flag,model_name,cv,oversampling,model_name='attn_icu_read',train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-factor",
   "metadata": {},
   "source": [
    "## 10. Running BEHRT\n",
    "Below we integrate the implementation of BEHRT in our pipeline.\n",
    "We perform pre-procesing needed to run BEHRT model. https://github.com/deepmedicine/BEHRT\n",
    "\n",
    "Few things to note before running BEHRT -\n",
    "- The numerical values are binned into quantiles.\n",
    "- BEHRT has recommended maximum number of events per sample to be 512. \n",
    "    So feature selection is important so that number of events per sample does not exceed 512.\n",
    "- The model is quite computationally heavy so it requires a GPU.\n",
    "\n",
    "The output files for BEHRT will be saved in ./data/behrt/ folder\n",
    "\n",
    "**Please run below cell to to pre-process and run BEHRT on the selected cohort**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc8e396d-de14-49eb-8f6e-8589da45997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(tokenization)\n",
    "import tokenization\n",
    "from tokenization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e47fe094-ec05-48c9-a0c1-5264e93c9aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_icu = True\n",
    "diag_flag = True\n",
    "proc_flag = False\n",
    "out_flag = False\n",
    "chart_flag = True\n",
    "med_flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aggressive-break",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING READING FILES.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [12:53<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED READING FILES. \n",
      "\n",
      "STARTING TOKENIZATION.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [10:23<00:00,  8.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED TOKENIZATION. \n",
      "\n",
      "FINAL COHORT STATISTICS: \n",
      "580 Positive samples.\n",
      "4420 Negative samples.\n",
      "\n",
      "2136 Female samples.\n",
      "2864 Male samples.\n",
      "\n",
      "416 BLACK/AFRICAN AMERICAN samples.\n",
      "3167 WHITE samples.\n",
      "531 UNKNOWN samples.\n",
      "33 BLACK/CAPE VERDEAN samples.\n",
      "10 WHITE - BRAZILIAN samples.\n",
      "20 BLACK/AFRICAN samples.\n",
      "34 HISPANIC OR LATINO samples.\n",
      "179 OTHER samples.\n",
      "63 UNABLE TO OBTAIN samples.\n",
      "3 HISPANIC/LATINO - SALVADORAN samples.\n",
      "85 WHITE - OTHER EUROPEAN samples.\n",
      "32 BLACK/CARIBBEAN ISLAND samples.\n",
      "68 ASIAN samples.\n",
      "53 HISPANIC/LATINO - DOMINICAN samples.\n",
      "69 HISPANIC/LATINO - PUERTO RICAN samples.\n",
      "43 WHITE - RUSSIAN samples.\n",
      "4 ASIAN - KOREAN samples.\n",
      "46 ASIAN - CHINESE samples.\n",
      "21 PATIENT DECLINED TO ANSWER samples.\n",
      "14 WHITE - EASTERN EUROPEAN samples.\n",
      "13 ASIAN - ASIAN INDIAN samples.\n",
      "11 ASIAN - SOUTH EAST ASIAN samples.\n",
      "8 NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER samples.\n",
      "9 AMERICAN INDIAN/ALASKA NATIVE samples.\n",
      "5 HISPANIC/LATINO - CENTRAL AMERICAN samples.\n",
      "35 PORTUGUESE samples.\n",
      "5 HISPANIC/LATINO - GUATEMALAN samples.\n",
      "5 HISPANIC/LATINO - HONDURAN samples.\n",
      "6 HISPANIC/LATINO - CUBAN samples.\n",
      "6 SOUTH AMERICAN samples.\n",
      "1 MULTIPLE RACE/ETHNICITY samples.\n",
      "4 HISPANIC/LATINO - COLUMBIAN samples.\n",
      "1 HISPANIC/LATINO - MEXICAN samples.\n",
      "\n",
      "\n",
      "2317 Medicare samples.\n",
      "2321 Other samples.\n",
      "362 Medicaid samples.\n",
      "Epoch n0\n",
      "0.6583743095397949\n",
      "TOTAL LOSS 12.049764975905418\n",
      "TRAIN 0.37709588618719414\t32.47795605659485 secs\n",
      "\n",
      "EVAL 0.3765551554970443\t2.342085838317871 secs\n",
      "\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "Epoch n1\n",
      "0.22817689180374146\n",
      "TOTAL LOSS 11.524342261254787\n",
      "TRAIN 0.35245235216672016\t34.2759211063385 secs\n",
      "\n",
      "EVAL 0.3601356956642121\t2.3383145332336426 secs\n",
      "\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "Epoch n2\n",
      "0.3812587261199951\n",
      "TOTAL LOSS 11.552172243595123\n",
      "TRAIN 0.347308734066138\t35.0578727722168 secs\n",
      "\n",
      "EVAL 0.3610053826123476\t2.3359522819519043 secs\n",
      "\n",
      "Epoch n3\n",
      "0.39952608942985535\n",
      "TOTAL LOSS 12.229155167937279\n",
      "TRAIN 0.33683048753433575\t34.314698457717896 secs\n",
      "\n",
      "EVAL 0.38216109899803996\t2.3235979080200195 secs\n",
      "\n",
      "Epoch n4\n",
      "0.34180206060409546\n",
      "TOTAL LOSS 11.245338901877403\n",
      "TRAIN 0.31929356213573995\t34.11225938796997 secs\n",
      "\n",
      "EVAL 0.35141684068366885\t2.3252573013305664 secs\n",
      "\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "Epoch n5\n",
      "0.34403157234191895\n",
      "TOTAL LOSS 11.137558411806822\n",
      "TRAIN 0.2822867924832318\t34.46396613121033 secs\n",
      "\n",
      "EVAL 0.3480487003689632\t2.3426554203033447 secs\n",
      "\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "Epoch n6\n",
      "0.03190279006958008\n",
      "TOTAL LOSS 11.286286644637585\n",
      "TRAIN 0.2619063417388968\t33.302799701690674 secs\n",
      "\n",
      "EVAL 0.3526964576449245\t2.3376424312591553 secs\n",
      "\n",
      "Epoch n7\n",
      "0.17424795031547546\n",
      "TOTAL LOSS 10.786991138011217\n",
      "TRAIN 0.2453089371561732\t33.95431709289551 secs\n",
      "\n",
      "EVAL 0.33709347306285053\t2.3251824378967285 secs\n",
      "\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "Epoch n8\n",
      "0.14650456607341766\n",
      "TOTAL LOSS 10.831548038870096\n",
      "TRAIN 0.2314227462138079\t34.250476360321045 secs\n",
      "\n",
      "EVAL 0.3384858762146905\t2.326202392578125 secs\n",
      "\n",
      "Epoch n9\n",
      "0.10350920259952545\n",
      "TOTAL LOSS 10.940365053713322\n",
      "TRAIN 0.20568440769497118\t33.744279623031616 secs\n",
      "\n",
      "EVAL 0.3418864079285413\t2.3492448329925537 secs\n",
      "\n",
      "Epoch n10\n",
      "0.18985430896282196\n",
      "TOTAL LOSS 11.152315318584442\n",
      "TRAIN 0.19054623419359396\t33.115188121795654 secs\n",
      "\n",
      "EVAL 0.3485098537057638\t2.355546236038208 secs\n",
      "\n",
      "Epoch n11\n",
      "0.06540583074092865\n",
      "TOTAL LOSS 11.304107505828142\n",
      "TRAIN 0.17676961519776685\t33.177701473236084 secs\n",
      "\n",
      "EVAL 0.35325335955712944\t2.3313536643981934 secs\n",
      "\n",
      "Epoch n12\n",
      "0.05909452214837074\n",
      "TOTAL LOSS 11.920239970088005\n",
      "TRAIN 0.16546888226753773\t33.46961045265198 secs\n",
      "\n",
      "EVAL 0.37250749906525016\t2.3332715034484863 secs\n",
      "\n",
      "Epoch n13\n",
      "0.13004973530769348\n",
      "TOTAL LOSS 12.380950778722763\n",
      "TRAIN 0.15180445019768118\t33.34729266166687 secs\n",
      "\n",
      "EVAL 0.38690471183508635\t2.326686382293701 secs\n",
      "\n",
      "Epoch n14\n",
      "0.0949563980102539\n",
      "TOTAL LOSS 12.287337243556976\n",
      "TRAIN 0.15120068820167895\t33.059778928756714 secs\n",
      "\n",
      "EVAL 0.3839792888611555\t2.337611436843872 secs\n",
      "\n",
      "Epoch n15\n",
      "0.026202552020549774\n",
      "TOTAL LOSS 14.476398587226868\n",
      "TRAIN 0.1327598501362553\t32.8092679977417 secs\n",
      "\n",
      "EVAL 0.4523874558508396\t2.3160629272460938 secs\n",
      "\n",
      "Epoch n16\n",
      "0.23692360520362854\n",
      "TOTAL LOSS 13.278819805011153\n",
      "TRAIN 0.12922007625829005\t32.6279182434082 secs\n",
      "\n",
      "EVAL 0.41496311890659854\t2.3459908962249756 secs\n",
      "\n",
      "Epoch n17\n",
      "0.05209387093782425\n",
      "TOTAL LOSS 13.411298074293882\n",
      "TRAIN 0.1159595532691506\t33.66638231277466 secs\n",
      "\n",
      "EVAL 0.4191030648216838\t2.346201181411743 secs\n",
      "\n",
      "Epoch n18\n",
      "0.07398499548435211\n",
      "TOTAL LOSS 14.124984245747328\n",
      "TRAIN 0.10930481391903608\t33.13010549545288 secs\n",
      "\n",
      "EVAL 0.441405757679604\t2.292478322982788 secs\n",
      "\n",
      "Epoch n19\n",
      "0.04965201020240784\n",
      "TOTAL LOSS 14.897148523945361\n",
      "TRAIN 0.10425171750644434\t33.68990230560303 secs\n",
      "\n",
      "EVAL 0.46553589137329254\t2.3163723945617676 secs\n",
      "\n",
      "Epoch n20\n",
      "0.03901413455605507\n",
      "TOTAL LOSS 14.531227357219905\n",
      "TRAIN 0.09537155856484691\t32.92587399482727 secs\n",
      "\n",
      "EVAL 0.454100854913122\t2.3183505535125732 secs\n",
      "\n",
      "Epoch n21\n",
      "0.023867184296250343\n",
      "TOTAL LOSS 16.40470201522112\n",
      "TRAIN 0.0807798455807393\t32.9205527305603 secs\n",
      "\n",
      "EVAL 0.51264693797566\t2.319855213165283 secs\n",
      "\n",
      "Epoch n22\n",
      "0.03946575149893761\n",
      "TOTAL LOSS 17.278852006420493\n",
      "TRAIN 0.07781230785871206\t34.086413860321045 secs\n",
      "\n",
      "EVAL 0.5399641252006404\t2.3258683681488037 secs\n",
      "\n",
      "Epoch n23\n",
      "0.40780699253082275\n",
      "TOTAL LOSS 16.620515076443553\n",
      "TRAIN 0.08079261984676123\t34.04243874549866 secs\n",
      "\n",
      "EVAL 0.519391096138861\t2.3704066276550293 secs\n",
      "\n",
      "Epoch n24\n",
      "0.009931560605764389\n",
      "TOTAL LOSS 17.371477518230677\n",
      "TRAIN 0.0714335179617671\t33.98803949356079 secs\n",
      "\n",
      "EVAL 0.5428586724447086\t2.3591580390930176 secs\n",
      "\n",
      "Epoch n25\n",
      "0.07968272268772125\n",
      "TOTAL LOSS 17.285181728191674\n",
      "TRAIN 0.058966773087966794\t33.38492774963379 secs\n",
      "\n",
      "EVAL 0.5401619290059898\t2.3519210815429688 secs\n",
      "\n",
      "Epoch n26\n",
      "0.021971462294459343\n",
      "TOTAL LOSS 18.365427541313693\n",
      "TRAIN 0.05611861786235243\t34.11958169937134 secs\n",
      "\n",
      "EVAL 0.5739196106660529\t2.3376121520996094 secs\n",
      "\n",
      "Epoch n27\n",
      "0.008461154997348785\n",
      "TOTAL LOSS 19.291414806619287\n",
      "TRAIN 0.049580449628596916\t33.60772633552551 secs\n",
      "\n",
      "EVAL 0.6028567127068527\t2.3374743461608887 secs\n",
      "\n",
      "Epoch n28\n",
      "0.013817720115184784\n",
      "TOTAL LOSS 18.10316527634859\n",
      "TRAIN 0.05687862838252732\t34.467089891433716 secs\n",
      "\n",
      "EVAL 0.5657239148858935\t2.3735244274139404 secs\n",
      "\n",
      "Epoch n29\n",
      "0.11458244174718857\n",
      "TOTAL LOSS 19.74059320241213\n",
      "TRAIN 0.04961433871138892\t34.25725960731506 secs\n",
      "\n",
      "EVAL 0.616893537575379\t2.3770039081573486 secs\n",
      "\n",
      "Epoch n30\n",
      "0.005949989892542362\n",
      "TOTAL LOSS 22.406670801341534\n",
      "TRAIN 0.044649847134093\t33.79222393035889 secs\n",
      "\n",
      "EVAL 0.7002084625419229\t2.3691883087158203 secs\n",
      "\n",
      "Epoch n31\n",
      "0.08275584131479263\n",
      "TOTAL LOSS 22.342325610574335\n",
      "TRAIN 0.040292291440597\t34.493130683898926 secs\n",
      "\n",
      "EVAL 0.698197675330448\t2.3769872188568115 secs\n",
      "\n",
      "Epoch n32\n",
      "0.007999557070434093\n",
      "TOTAL LOSS 19.88676266046241\n",
      "TRAIN 0.03929809217190276\t34.06580972671509 secs\n",
      "\n",
      "EVAL 0.6214613331394503\t2.3647499084472656 secs\n",
      "\n",
      "Epoch n33\n",
      "0.006416198797523975\n",
      "TOTAL LOSS 23.612575673265383\n",
      "TRAIN 0.036135258744194355\t34.13693118095398 secs\n",
      "\n",
      "EVAL 0.7378929897895432\t2.406034469604492 secs\n",
      "\n",
      "Epoch n34\n",
      "0.0028596222400665283\n",
      "TOTAL LOSS 21.799914159346372\n",
      "TRAIN 0.03375513467099943\t32.24882984161377 secs\n",
      "\n",
      "EVAL 0.6812473174795741\t2.410017967224121 secs\n",
      "\n",
      "Epoch n35\n",
      "0.018820947036147118\n",
      "TOTAL LOSS 21.808765906607732\n",
      "TRAIN 0.035236951480959816\t32.88691973686218 secs\n",
      "\n",
      "EVAL 0.6815239345814916\t2.4326095581054688 secs\n",
      "\n",
      "Epoch n36\n",
      "0.004719659686088562\n",
      "TOTAL LOSS 22.899018028983846\n",
      "TRAIN 0.022289179185762592\t33.54111313819885 secs\n",
      "\n",
      "EVAL 0.7155943134057452\t2.414231300354004 secs\n",
      "\n",
      "Epoch n37\n",
      "0.0019154609180986881\n",
      "TOTAL LOSS 23.515844554640353\n",
      "TRAIN 0.027671797226553094\t32.3307158946991 secs\n",
      "\n",
      "EVAL 0.734870142332511\t2.3735013008117676 secs\n",
      "\n",
      "Epoch n38\n",
      "0.0024778610095381737\n",
      "TOTAL LOSS 21.84954379685223\n",
      "TRAIN 0.03255230053615029\t32.25877118110657 secs\n",
      "\n",
      "EVAL 0.6827982436516322\t2.3703320026397705 secs\n",
      "\n",
      "Epoch n39\n",
      "0.006001397501677275\n",
      "TOTAL LOSS 22.393382601439953\n",
      "TRAIN 0.028555771769194535\t32.31083536148071 secs\n",
      "\n",
      "EVAL 0.6997932062949985\t2.369680881500244 secs\n",
      "\n",
      "Epoch n40\n",
      "0.03083425760269165\n",
      "TOTAL LOSS 24.04044207977131\n",
      "TRAIN 0.02088964865202739\t32.24771857261658 secs\n",
      "\n",
      "EVAL 0.7512638149928534\t2.391920566558838 secs\n",
      "\n",
      "Epoch n41\n",
      "0.0019424197962507606\n",
      "TOTAL LOSS 23.390534331672825\n",
      "TRAIN 0.024150630176124763\t32.22526574134827 secs\n",
      "\n",
      "EVAL 0.7309541978647758\t2.3721275329589844 secs\n",
      "\n",
      "Epoch n42\n",
      "0.001771492650732398\n",
      "TOTAL LOSS 23.56584313325584\n",
      "TRAIN 0.02526597863171658\t32.27822542190552 secs\n",
      "\n",
      "EVAL 0.736432597914245\t2.3860621452331543 secs\n",
      "\n",
      "Epoch n43\n",
      "0.0017444031545892358\n",
      "TOTAL LOSS 23.9842216540128\n",
      "TRAIN 0.018674785380526\t34.60919523239136 secs\n",
      "\n",
      "EVAL 0.7495069266879\t2.4118337631225586 secs\n",
      "\n",
      "Epoch n44\n",
      "0.018906794488430023\n",
      "TOTAL LOSS 24.438203294179402\n",
      "TRAIN 0.019628470257768774\t33.940269470214844 secs\n",
      "\n",
      "EVAL 0.7636938529431063\t2.376384735107422 secs\n",
      "\n",
      "Epoch n45\n",
      "0.0017036549979820848\n",
      "TOTAL LOSS 25.069667452946305\n",
      "TRAIN 0.020064290475257627\t33.774068117141724 secs\n",
      "\n",
      "EVAL 0.783427107904572\t2.3549203872680664 secs\n",
      "\n",
      "Epoch n46\n",
      "0.0007783639011904597\n",
      "TOTAL LOSS 24.840882597258314\n",
      "TRAIN 0.020070464723981645\t32.550769329071045 secs\n",
      "\n",
      "EVAL 0.7762775811643223\t2.3797659873962402 secs\n",
      "\n",
      "Epoch n47\n",
      "0.0022706459276378155\n",
      "TOTAL LOSS 25.66244831046788\n",
      "TRAIN 0.01507886105241492\t32.89166593551636 secs\n",
      "\n",
      "EVAL 0.8019515097021213\t2.348567247390747 secs\n",
      "\n",
      "Epoch n48\n",
      "0.0016435454599559307\n",
      "TOTAL LOSS 26.752551479788963\n",
      "TRAIN 0.01576678709687291\t33.35542845726013 secs\n",
      "\n",
      "EVAL 0.8360172337434051\t2.3422820568084717 secs\n",
      "\n",
      "Epoch n49\n",
      "0.0026285506319254637\n",
      "TOTAL LOSS 27.519195578992367\n",
      "TRAIN 0.013782637561118596\t34.12050104141235 secs\n",
      "\n",
      "EVAL 0.8599748618435115\t2.3845386505126953 secs\n",
      "\n",
      "Loading succesfull\n",
      "TOTAL LOSS 18.40738444775343\n",
      "tensor([[0.0807],\n",
      "        [0.7950],\n",
      "        [0.0304],\n",
      "        [0.0207],\n",
      "        [0.0238],\n",
      "        [0.0148],\n",
      "        [0.2176],\n",
      "        [0.0183],\n",
      "        [0.0241],\n",
      "        [0.0360],\n",
      "        [0.0213],\n",
      "        [0.2410],\n",
      "        [0.0247],\n",
      "        [0.0173],\n",
      "        [0.0157],\n",
      "        [0.0336],\n",
      "        [0.0158],\n",
      "        [0.0327],\n",
      "        [0.0152],\n",
      "        [0.0408],\n",
      "        [0.0264],\n",
      "        [0.0450],\n",
      "        [0.0421],\n",
      "        [0.0150],\n",
      "        [0.0134],\n",
      "        [0.0235],\n",
      "        [0.0466],\n",
      "        [0.0149],\n",
      "        [0.0137],\n",
      "        [0.0167],\n",
      "        [0.0278],\n",
      "        [0.0483],\n",
      "        [0.0135],\n",
      "        [0.0163],\n",
      "        [0.0246],\n",
      "        [0.0243],\n",
      "        [0.5074],\n",
      "        [0.0325],\n",
      "        [0.0138],\n",
      "        [0.0492],\n",
      "        [0.0361],\n",
      "        [0.0349],\n",
      "        [0.0143],\n",
      "        [0.0150],\n",
      "        [0.3310],\n",
      "        [0.0155],\n",
      "        [0.1324],\n",
      "        [0.7218],\n",
      "        [0.0404],\n",
      "        [0.0241],\n",
      "        [0.0213],\n",
      "        [0.0174],\n",
      "        [0.0149],\n",
      "        [0.1282],\n",
      "        [0.7994],\n",
      "        [0.4799],\n",
      "        [0.0202],\n",
      "        [0.0165],\n",
      "        [0.0152],\n",
      "        [0.0209],\n",
      "        [0.0470],\n",
      "        [0.4175],\n",
      "        [0.0165],\n",
      "        [0.0161],\n",
      "        [0.0189],\n",
      "        [0.1397],\n",
      "        [0.1450],\n",
      "        [0.3273],\n",
      "        [0.0143],\n",
      "        [0.0157],\n",
      "        [0.0221],\n",
      "        [0.0582],\n",
      "        [0.0145],\n",
      "        [0.0754],\n",
      "        [0.0149],\n",
      "        [0.0149],\n",
      "        [0.0966],\n",
      "        [0.0145],\n",
      "        [0.0728],\n",
      "        [0.0480],\n",
      "        [0.0155],\n",
      "        [0.0139],\n",
      "        [0.0151],\n",
      "        [0.0138],\n",
      "        [0.0135],\n",
      "        [0.0208],\n",
      "        [0.6066],\n",
      "        [0.1048],\n",
      "        [0.0145],\n",
      "        [0.1422],\n",
      "        [0.0433],\n",
      "        [0.7487],\n",
      "        [0.0277],\n",
      "        [0.1836],\n",
      "        [0.0595],\n",
      "        [0.0158],\n",
      "        [0.0146],\n",
      "        [0.0147],\n",
      "        [0.0769],\n",
      "        [0.0159],\n",
      "        [0.9010],\n",
      "        [0.0343],\n",
      "        [0.1120],\n",
      "        [0.0195],\n",
      "        [0.0354],\n",
      "        [0.0336],\n",
      "        [0.0625],\n",
      "        [0.9069],\n",
      "        [0.0241],\n",
      "        [0.0213],\n",
      "        [0.1382],\n",
      "        [0.0243],\n",
      "        [0.1876],\n",
      "        [0.1490],\n",
      "        [0.0215],\n",
      "        [0.0139],\n",
      "        [0.0791],\n",
      "        [0.0142],\n",
      "        [0.0541],\n",
      "        [0.0248],\n",
      "        [0.0140],\n",
      "        [0.0193],\n",
      "        [0.3093],\n",
      "        [0.0262],\n",
      "        [0.0212],\n",
      "        [0.0191],\n",
      "        [0.1733],\n",
      "        [0.0388],\n",
      "        [0.0265],\n",
      "        [0.0191],\n",
      "        [0.4080],\n",
      "        [0.0171],\n",
      "        [0.0178],\n",
      "        [0.0686],\n",
      "        [0.0166],\n",
      "        [0.0178],\n",
      "        [0.0142],\n",
      "        [0.3312],\n",
      "        [0.0427],\n",
      "        [0.0804],\n",
      "        [0.0367],\n",
      "        [0.0192],\n",
      "        [0.1836],\n",
      "        [0.0277],\n",
      "        [0.1028],\n",
      "        [0.0238],\n",
      "        [0.1322],\n",
      "        [0.3560],\n",
      "        [0.0189],\n",
      "        [0.0198],\n",
      "        [0.0175],\n",
      "        [0.0768],\n",
      "        [0.0162],\n",
      "        [0.0195],\n",
      "        [0.0162],\n",
      "        [0.0512],\n",
      "        [0.0844],\n",
      "        [0.0162],\n",
      "        [0.7401],\n",
      "        [0.0134],\n",
      "        [0.0193],\n",
      "        [0.0642],\n",
      "        [0.0161],\n",
      "        [0.0211],\n",
      "        [0.0250],\n",
      "        [0.0641],\n",
      "        [0.0386],\n",
      "        [0.1499],\n",
      "        [0.0149],\n",
      "        [0.0211],\n",
      "        [0.1556],\n",
      "        [0.3999],\n",
      "        [0.0246],\n",
      "        [0.1716],\n",
      "        [0.0535],\n",
      "        [0.0221],\n",
      "        [0.1144],\n",
      "        [0.0221],\n",
      "        [0.0208],\n",
      "        [0.3528],\n",
      "        [0.6276],\n",
      "        [0.0168],\n",
      "        [0.4016],\n",
      "        [0.1289],\n",
      "        [0.0162],\n",
      "        [0.0256],\n",
      "        [0.8740],\n",
      "        [0.0144],\n",
      "        [0.0401],\n",
      "        [0.0137],\n",
      "        [0.0261],\n",
      "        [0.8504],\n",
      "        [0.0272],\n",
      "        [0.1984],\n",
      "        [0.0835],\n",
      "        [0.0144],\n",
      "        [0.0284],\n",
      "        [0.6438],\n",
      "        [0.0200],\n",
      "        [0.1114],\n",
      "        [0.0413],\n",
      "        [0.0862],\n",
      "        [0.0165],\n",
      "        [0.4757],\n",
      "        [0.0480],\n",
      "        [0.0333],\n",
      "        [0.0307],\n",
      "        [0.0159],\n",
      "        [0.0611],\n",
      "        [0.0573],\n",
      "        [0.0262],\n",
      "        [0.0451],\n",
      "        [0.0149],\n",
      "        [0.0137],\n",
      "        [0.0215],\n",
      "        [0.2227],\n",
      "        [0.0401],\n",
      "        [0.0163],\n",
      "        [0.0307],\n",
      "        [0.0172],\n",
      "        [0.0206],\n",
      "        [0.0166],\n",
      "        [0.0466],\n",
      "        [0.0155],\n",
      "        [0.0673],\n",
      "        [0.0598],\n",
      "        [0.1323],\n",
      "        [0.0534],\n",
      "        [0.1322],\n",
      "        [0.1199],\n",
      "        [0.0166],\n",
      "        [0.0179],\n",
      "        [0.0301],\n",
      "        [0.1433],\n",
      "        [0.0171],\n",
      "        [0.0186],\n",
      "        [0.1869],\n",
      "        [0.1471],\n",
      "        [0.0145],\n",
      "        [0.0148],\n",
      "        [0.0687],\n",
      "        [0.0160],\n",
      "        [0.1881],\n",
      "        [0.4478],\n",
      "        [0.0149],\n",
      "        [0.5519],\n",
      "        [0.0233],\n",
      "        [0.8420],\n",
      "        [0.0711],\n",
      "        [0.2262],\n",
      "        [0.0144],\n",
      "        [0.0142],\n",
      "        [0.0143],\n",
      "        [0.0238],\n",
      "        [0.0142],\n",
      "        [0.0408],\n",
      "        [0.0155],\n",
      "        [0.0274],\n",
      "        [0.0139],\n",
      "        [0.0148],\n",
      "        [0.0139],\n",
      "        [0.4755],\n",
      "        [0.1615],\n",
      "        [0.0136],\n",
      "        [0.0141],\n",
      "        [0.0555],\n",
      "        [0.0163],\n",
      "        [0.2335],\n",
      "        [0.8638],\n",
      "        [0.0282],\n",
      "        [0.0266],\n",
      "        [0.3457],\n",
      "        [0.0132],\n",
      "        [0.0399],\n",
      "        [0.1055],\n",
      "        [0.0221],\n",
      "        [0.0189],\n",
      "        [0.0219],\n",
      "        [0.0150],\n",
      "        [0.0660],\n",
      "        [0.0139],\n",
      "        [0.6450],\n",
      "        [0.0178],\n",
      "        [0.0238],\n",
      "        [0.0144],\n",
      "        [0.6932],\n",
      "        [0.0164],\n",
      "        [0.0245],\n",
      "        [0.0160],\n",
      "        [0.0173],\n",
      "        [0.0480],\n",
      "        [0.0330],\n",
      "        [0.0181],\n",
      "        [0.0341],\n",
      "        [0.1129],\n",
      "        [0.0187],\n",
      "        [0.0199],\n",
      "        [0.0157],\n",
      "        [0.0174],\n",
      "        [0.0260],\n",
      "        [0.8133],\n",
      "        [0.0152],\n",
      "        [0.2984],\n",
      "        [0.3331],\n",
      "        [0.0182],\n",
      "        [0.0173],\n",
      "        [0.3373],\n",
      "        [0.1069],\n",
      "        [0.0228],\n",
      "        [0.0154],\n",
      "        [0.0193],\n",
      "        [0.0156],\n",
      "        [0.0183],\n",
      "        [0.0189],\n",
      "        [0.0152],\n",
      "        [0.0688],\n",
      "        [0.0139],\n",
      "        [0.0165],\n",
      "        [0.0164],\n",
      "        [0.0146],\n",
      "        [0.0335],\n",
      "        [0.1448],\n",
      "        [0.0184],\n",
      "        [0.0657],\n",
      "        [0.0228],\n",
      "        [0.0144],\n",
      "        [0.0161],\n",
      "        [0.2758],\n",
      "        [0.5582],\n",
      "        [0.1230],\n",
      "        [0.3371],\n",
      "        [0.0280],\n",
      "        [0.0157],\n",
      "        [0.0385],\n",
      "        [0.0393],\n",
      "        [0.0158],\n",
      "        [0.0169],\n",
      "        [0.0138],\n",
      "        [0.3695],\n",
      "        [0.0593],\n",
      "        [0.0143],\n",
      "        [0.4150],\n",
      "        [0.0412],\n",
      "        [0.6126],\n",
      "        [0.0535],\n",
      "        [0.0149],\n",
      "        [0.0334],\n",
      "        [0.0156],\n",
      "        [0.0194],\n",
      "        [0.0744],\n",
      "        [0.6292],\n",
      "        [0.0141],\n",
      "        [0.0772],\n",
      "        [0.4750],\n",
      "        [0.0153],\n",
      "        [0.0783],\n",
      "        [0.0139],\n",
      "        [0.0201],\n",
      "        [0.5931],\n",
      "        [0.0144],\n",
      "        [0.0589],\n",
      "        [0.0182],\n",
      "        [0.3795],\n",
      "        [0.0182],\n",
      "        [0.0151],\n",
      "        [0.0610],\n",
      "        [0.0806],\n",
      "        [0.0371],\n",
      "        [0.7439],\n",
      "        [0.1057],\n",
      "        [0.0663],\n",
      "        [0.0293],\n",
      "        [0.3121],\n",
      "        [0.0464],\n",
      "        [0.0152],\n",
      "        [0.0153],\n",
      "        [0.6252],\n",
      "        [0.0249],\n",
      "        [0.0162],\n",
      "        [0.4714],\n",
      "        [0.3833],\n",
      "        [0.0245],\n",
      "        [0.0164],\n",
      "        [0.0332],\n",
      "        [0.0171],\n",
      "        [0.0152],\n",
      "        [0.7596],\n",
      "        [0.0277],\n",
      "        [0.0149],\n",
      "        [0.0155],\n",
      "        [0.2634],\n",
      "        [0.0235],\n",
      "        [0.0435],\n",
      "        [0.7299],\n",
      "        [0.0559],\n",
      "        [0.0244],\n",
      "        [0.0260],\n",
      "        [0.0178],\n",
      "        [0.0172],\n",
      "        [0.1077],\n",
      "        [0.0142],\n",
      "        [0.0161],\n",
      "        [0.1629],\n",
      "        [0.0151],\n",
      "        [0.0211],\n",
      "        [0.2559],\n",
      "        [0.0258],\n",
      "        [0.0145],\n",
      "        [0.0172],\n",
      "        [0.0183],\n",
      "        [0.0144],\n",
      "        [0.0183],\n",
      "        [0.2297],\n",
      "        [0.2266],\n",
      "        [0.0455],\n",
      "        [0.7121],\n",
      "        [0.4250],\n",
      "        [0.0172],\n",
      "        [0.0155],\n",
      "        [0.0908],\n",
      "        [0.2124],\n",
      "        [0.0441],\n",
      "        [0.1856],\n",
      "        [0.0152],\n",
      "        [0.5492],\n",
      "        [0.2598],\n",
      "        [0.0143],\n",
      "        [0.0598],\n",
      "        [0.0186],\n",
      "        [0.5963],\n",
      "        [0.0191],\n",
      "        [0.0141],\n",
      "        [0.0200],\n",
      "        [0.0138],\n",
      "        [0.0385],\n",
      "        [0.0463],\n",
      "        [0.0181],\n",
      "        [0.1193],\n",
      "        [0.1160],\n",
      "        [0.0879],\n",
      "        [0.1392],\n",
      "        [0.1275],\n",
      "        [0.0238],\n",
      "        [0.0143],\n",
      "        [0.0135],\n",
      "        [0.0718],\n",
      "        [0.2950],\n",
      "        [0.1233],\n",
      "        [0.0197],\n",
      "        [0.0148],\n",
      "        [0.1333],\n",
      "        [0.0272],\n",
      "        [0.1978],\n",
      "        [0.0459],\n",
      "        [0.0373],\n",
      "        [0.0445],\n",
      "        [0.6603],\n",
      "        [0.0349],\n",
      "        [0.0142],\n",
      "        [0.0213],\n",
      "        [0.0137],\n",
      "        [0.0187],\n",
      "        [0.0656],\n",
      "        [0.0219],\n",
      "        [0.0151],\n",
      "        [0.0137],\n",
      "        [0.0151],\n",
      "        [0.0221],\n",
      "        [0.0163],\n",
      "        [0.3742],\n",
      "        [0.4635],\n",
      "        [0.0175],\n",
      "        [0.0302],\n",
      "        [0.6190],\n",
      "        [0.2482],\n",
      "        [0.0228],\n",
      "        [0.3214],\n",
      "        [0.0164],\n",
      "        [0.0145],\n",
      "        [0.0215],\n",
      "        [0.3191],\n",
      "        [0.2027],\n",
      "        [0.4362],\n",
      "        [0.1106],\n",
      "        [0.2487],\n",
      "        [0.0166],\n",
      "        [0.0168],\n",
      "        [0.2529],\n",
      "        [0.0144],\n",
      "        [0.0134],\n",
      "        [0.1618],\n",
      "        [0.5382],\n",
      "        [0.5011],\n",
      "        [0.0158],\n",
      "        [0.0210],\n",
      "        [0.0273],\n",
      "        [0.0224],\n",
      "        [0.0156],\n",
      "        [0.0745],\n",
      "        [0.0244],\n",
      "        [0.0161],\n",
      "        [0.0248],\n",
      "        [0.0152],\n",
      "        [0.0150],\n",
      "        [0.0235],\n",
      "        [0.0181],\n",
      "        [0.0140],\n",
      "        [0.0171],\n",
      "        [0.7331],\n",
      "        [0.6416],\n",
      "        [0.0136],\n",
      "        [0.0283],\n",
      "        [0.0146],\n",
      "        [0.0380],\n",
      "        [0.0601],\n",
      "        [0.0150],\n",
      "        [0.0171],\n",
      "        [0.0274],\n",
      "        [0.0134],\n",
      "        [0.0155],\n",
      "        [0.0135],\n",
      "        [0.0137],\n",
      "        [0.0421],\n",
      "        [0.4884],\n",
      "        [0.0799],\n",
      "        [0.4694],\n",
      "        [0.0267],\n",
      "        [0.0361],\n",
      "        [0.1099],\n",
      "        [0.0142],\n",
      "        [0.0410],\n",
      "        [0.0214],\n",
      "        [0.0143],\n",
      "        [0.4699],\n",
      "        [0.0206],\n",
      "        [0.0198],\n",
      "        [0.4179],\n",
      "        [0.0871],\n",
      "        [0.4992],\n",
      "        [0.0608],\n",
      "        [0.1447],\n",
      "        [0.0207],\n",
      "        [0.0883],\n",
      "        [0.1744],\n",
      "        [0.0148],\n",
      "        [0.0151],\n",
      "        [0.2638],\n",
      "        [0.3480],\n",
      "        [0.1280],\n",
      "        [0.0148],\n",
      "        [0.4034],\n",
      "        [0.0206],\n",
      "        [0.0855],\n",
      "        [0.0162],\n",
      "        [0.0243],\n",
      "        [0.0208],\n",
      "        [0.0159],\n",
      "        [0.0837],\n",
      "        [0.0144],\n",
      "        [0.6713],\n",
      "        [0.0349],\n",
      "        [0.0143],\n",
      "        [0.0238],\n",
      "        [0.0218],\n",
      "        [0.5130],\n",
      "        [0.2831],\n",
      "        [0.8031],\n",
      "        [0.0151],\n",
      "        [0.0160],\n",
      "        [0.0486],\n",
      "        [0.0364],\n",
      "        [0.0154],\n",
      "        [0.0180],\n",
      "        [0.7510],\n",
      "        [0.0989],\n",
      "        [0.4811],\n",
      "        [0.1234],\n",
      "        [0.6426],\n",
      "        [0.4283],\n",
      "        [0.0357],\n",
      "        [0.0162],\n",
      "        [0.0163],\n",
      "        [0.7608],\n",
      "        [0.0311],\n",
      "        [0.0162],\n",
      "        [0.0168],\n",
      "        [0.1144],\n",
      "        [0.0589],\n",
      "        [0.0155],\n",
      "        [0.0635],\n",
      "        [0.0185],\n",
      "        [0.0154],\n",
      "        [0.1157],\n",
      "        [0.0223],\n",
      "        [0.1133],\n",
      "        [0.1428],\n",
      "        [0.0778],\n",
      "        [0.0174],\n",
      "        [0.0333],\n",
      "        [0.0663],\n",
      "        [0.0552],\n",
      "        [0.0160],\n",
      "        [0.0763],\n",
      "        [0.5139],\n",
      "        [0.2812],\n",
      "        [0.0214],\n",
      "        [0.0154],\n",
      "        [0.0208],\n",
      "        [0.0461],\n",
      "        [0.0141],\n",
      "        [0.0283],\n",
      "        [0.0152],\n",
      "        [0.8542],\n",
      "        [0.0159],\n",
      "        [0.7073],\n",
      "        [0.0198],\n",
      "        [0.0455],\n",
      "        [0.0652],\n",
      "        [0.0727],\n",
      "        [0.1879],\n",
      "        [0.0159],\n",
      "        [0.0260],\n",
      "        [0.0356],\n",
      "        [0.0921],\n",
      "        [0.7116],\n",
      "        [0.0143],\n",
      "        [0.0223],\n",
      "        [0.0156],\n",
      "        [0.0276],\n",
      "        [0.0144],\n",
      "        [0.0162],\n",
      "        [0.0165],\n",
      "        [0.0435],\n",
      "        [0.0560],\n",
      "        [0.0165],\n",
      "        [0.0302],\n",
      "        [0.0154],\n",
      "        [0.2935],\n",
      "        [0.0153],\n",
      "        [0.0216],\n",
      "        [0.0182],\n",
      "        [0.0148],\n",
      "        [0.0944],\n",
      "        [0.0264],\n",
      "        [0.1738],\n",
      "        [0.0325],\n",
      "        [0.0151],\n",
      "        [0.6141],\n",
      "        [0.1843],\n",
      "        [0.5707],\n",
      "        [0.0170],\n",
      "        [0.0315],\n",
      "        [0.0208],\n",
      "        [0.0157],\n",
      "        [0.0137],\n",
      "        [0.0322],\n",
      "        [0.0199],\n",
      "        [0.2144],\n",
      "        [0.0400],\n",
      "        [0.0146],\n",
      "        [0.0159],\n",
      "        [0.0530],\n",
      "        [0.0275],\n",
      "        [0.0316],\n",
      "        [0.0146],\n",
      "        [0.0175],\n",
      "        [0.0150],\n",
      "        [0.0593],\n",
      "        [0.0170],\n",
      "        [0.3842],\n",
      "        [0.0449],\n",
      "        [0.0725],\n",
      "        [0.0326],\n",
      "        [0.0156],\n",
      "        [0.0136],\n",
      "        [0.1014],\n",
      "        [0.0157],\n",
      "        [0.0190],\n",
      "        [0.0153],\n",
      "        [0.0403],\n",
      "        [0.0438],\n",
      "        [0.0344],\n",
      "        [0.2257],\n",
      "        [0.8427],\n",
      "        [0.0351],\n",
      "        [0.1704],\n",
      "        [0.0222],\n",
      "        [0.1595],\n",
      "        [0.0140],\n",
      "        [0.4028],\n",
      "        [0.0141],\n",
      "        [0.0146],\n",
      "        [0.0147],\n",
      "        [0.3546],\n",
      "        [0.0136],\n",
      "        [0.0198],\n",
      "        [0.0271],\n",
      "        [0.0275],\n",
      "        [0.0183],\n",
      "        [0.0230],\n",
      "        [0.0351],\n",
      "        [0.0571],\n",
      "        [0.0161],\n",
      "        [0.0135],\n",
      "        [0.0143],\n",
      "        [0.1150],\n",
      "        [0.7256],\n",
      "        [0.6173],\n",
      "        [0.0180],\n",
      "        [0.0441],\n",
      "        [0.0307],\n",
      "        [0.0217],\n",
      "        [0.0209],\n",
      "        [0.0138],\n",
      "        [0.2553],\n",
      "        [0.0229],\n",
      "        [0.3237],\n",
      "        [0.1210],\n",
      "        [0.0263],\n",
      "        [0.7891],\n",
      "        [0.0668],\n",
      "        [0.0222],\n",
      "        [0.1336],\n",
      "        [0.0582],\n",
      "        [0.0166],\n",
      "        [0.4315],\n",
      "        [0.0297],\n",
      "        [0.0161],\n",
      "        [0.3469],\n",
      "        [0.0412],\n",
      "        [0.0259],\n",
      "        [0.4396],\n",
      "        [0.0300],\n",
      "        [0.5748],\n",
      "        [0.0255],\n",
      "        [0.2515],\n",
      "        [0.0137],\n",
      "        [0.1048],\n",
      "        [0.0137],\n",
      "        [0.0135],\n",
      "        [0.0155],\n",
      "        [0.2702],\n",
      "        [0.0174],\n",
      "        [0.7522],\n",
      "        [0.0863],\n",
      "        [0.1269],\n",
      "        [0.1510],\n",
      "        [0.0155],\n",
      "        [0.0266],\n",
      "        [0.0781],\n",
      "        [0.1481],\n",
      "        [0.2358],\n",
      "        [0.0132],\n",
      "        [0.0227],\n",
      "        [0.1294],\n",
      "        [0.0987],\n",
      "        [0.0133],\n",
      "        [0.0195],\n",
      "        [0.0777],\n",
      "        [0.0851],\n",
      "        [0.1958],\n",
      "        [0.0166],\n",
      "        [0.1007],\n",
      "        [0.1663],\n",
      "        [0.0356],\n",
      "        [0.0176],\n",
      "        [0.0225],\n",
      "        [0.0163],\n",
      "        [0.0158],\n",
      "        [0.0700],\n",
      "        [0.0548],\n",
      "        [0.0157],\n",
      "        [0.0179],\n",
      "        [0.0211],\n",
      "        [0.0142],\n",
      "        [0.0143],\n",
      "        [0.0285],\n",
      "        [0.0270],\n",
      "        [0.0405],\n",
      "        [0.0168],\n",
      "        [0.4915],\n",
      "        [0.0143],\n",
      "        [0.0934],\n",
      "        [0.0216],\n",
      "        [0.0146],\n",
      "        [0.0621],\n",
      "        [0.0260],\n",
      "        [0.0213],\n",
      "        [0.0150],\n",
      "        [0.0148],\n",
      "        [0.0185],\n",
      "        [0.8823],\n",
      "        [0.0306],\n",
      "        [0.4182],\n",
      "        [0.0650],\n",
      "        [0.0556],\n",
      "        [0.1975],\n",
      "        [0.0235],\n",
      "        [0.0150],\n",
      "        [0.0634],\n",
      "        [0.2495],\n",
      "        [0.0489],\n",
      "        [0.0177],\n",
      "        [0.0141],\n",
      "        [0.0227],\n",
      "        [0.0817],\n",
      "        [0.0146],\n",
      "        [0.6847],\n",
      "        [0.0171],\n",
      "        [0.0193],\n",
      "        [0.0247],\n",
      "        [0.6232],\n",
      "        [0.0297],\n",
      "        [0.0133],\n",
      "        [0.0165],\n",
      "        [0.0135],\n",
      "        [0.0520],\n",
      "        [0.1610],\n",
      "        [0.5775],\n",
      "        [0.0265],\n",
      "        [0.0549],\n",
      "        [0.0144],\n",
      "        [0.0467],\n",
      "        [0.5643],\n",
      "        [0.0247],\n",
      "        [0.0212],\n",
      "        [0.2461],\n",
      "        [0.0212],\n",
      "        [0.4861],\n",
      "        [0.0429],\n",
      "        [0.4628],\n",
      "        [0.1452],\n",
      "        [0.5543],\n",
      "        [0.0423],\n",
      "        [0.0374],\n",
      "        [0.0151],\n",
      "        [0.1350],\n",
      "        [0.0163],\n",
      "        [0.0627],\n",
      "        [0.0418],\n",
      "        [0.0136],\n",
      "        [0.0170],\n",
      "        [0.0134],\n",
      "        [0.0206],\n",
      "        [0.0175],\n",
      "        [0.0153],\n",
      "        [0.0467],\n",
      "        [0.0175],\n",
      "        [0.0155],\n",
      "        [0.0332],\n",
      "        [0.5654],\n",
      "        [0.0580],\n",
      "        [0.0152],\n",
      "        [0.0552],\n",
      "        [0.0169],\n",
      "        [0.3724],\n",
      "        [0.0262],\n",
      "        [0.1434],\n",
      "        [0.0180],\n",
      "        [0.0298],\n",
      "        [0.0133],\n",
      "        [0.1900],\n",
      "        [0.0981],\n",
      "        [0.0451],\n",
      "        [0.0840],\n",
      "        [0.0171],\n",
      "        [0.1194],\n",
      "        [0.2667],\n",
      "        [0.0141],\n",
      "        [0.2449],\n",
      "        [0.0202],\n",
      "        [0.0150],\n",
      "        [0.0184],\n",
      "        [0.0511],\n",
      "        [0.0151],\n",
      "        [0.6470],\n",
      "        [0.0195],\n",
      "        [0.1113],\n",
      "        [0.0144],\n",
      "        [0.6475],\n",
      "        [0.0153],\n",
      "        [0.0181],\n",
      "        [0.8131],\n",
      "        [0.0173],\n",
      "        [0.0140],\n",
      "        [0.0403],\n",
      "        [0.1183],\n",
      "        [0.3569],\n",
      "        [0.0137],\n",
      "        [0.0565],\n",
      "        [0.5728],\n",
      "        [0.2941],\n",
      "        [0.0163],\n",
      "        [0.5150],\n",
      "        [0.0391],\n",
      "        [0.0174],\n",
      "        [0.0590],\n",
      "        [0.7316],\n",
      "        [0.1433],\n",
      "        [0.0203],\n",
      "        [0.0235],\n",
      "        [0.0197],\n",
      "        [0.0147],\n",
      "        [0.2629],\n",
      "        [0.0135],\n",
      "        [0.0164],\n",
      "        [0.6373],\n",
      "        [0.0190],\n",
      "        [0.0182],\n",
      "        [0.2036],\n",
      "        [0.0184],\n",
      "        [0.0140],\n",
      "        [0.4814],\n",
      "        [0.0380],\n",
      "        [0.0623],\n",
      "        [0.0360],\n",
      "        [0.0361],\n",
      "        [0.1048],\n",
      "        [0.0319],\n",
      "        [0.0176],\n",
      "        [0.0971],\n",
      "        [0.0150],\n",
      "        [0.0149],\n",
      "        [0.0231],\n",
      "        [0.0135],\n",
      "        [0.0190],\n",
      "        [0.0162],\n",
      "        [0.0167],\n",
      "        [0.0150],\n",
      "        [0.0908],\n",
      "        [0.0180],\n",
      "        [0.5661],\n",
      "        [0.2779],\n",
      "        [0.0144],\n",
      "        [0.9314],\n",
      "        [0.6959],\n",
      "        [0.0203],\n",
      "        [0.3232],\n",
      "        [0.0146],\n",
      "        [0.1720],\n",
      "        [0.0155],\n",
      "        [0.0203],\n",
      "        [0.0502],\n",
      "        [0.0203],\n",
      "        [0.0238],\n",
      "        [0.0497],\n",
      "        [0.0214],\n",
      "        [0.1984],\n",
      "        [0.1819],\n",
      "        [0.0148],\n",
      "        [0.0214],\n",
      "        [0.0342],\n",
      "        [0.5779],\n",
      "        [0.1100],\n",
      "        [0.0173],\n",
      "        [0.0298],\n",
      "        [0.0216],\n",
      "        [0.1765],\n",
      "        [0.0258],\n",
      "        [0.0179],\n",
      "        [0.0156],\n",
      "        [0.0188],\n",
      "        [0.0134],\n",
      "        [0.0160],\n",
      "        [0.2126],\n",
      "        [0.0614],\n",
      "        [0.1395],\n",
      "        [0.0216],\n",
      "        [0.9201],\n",
      "        [0.4019],\n",
      "        [0.4047],\n",
      "        [0.7989],\n",
      "        [0.1418],\n",
      "        [0.0230],\n",
      "        [0.0260],\n",
      "        [0.0331],\n",
      "        [0.0201],\n",
      "        [0.0889],\n",
      "        [0.0149],\n",
      "        [0.0249],\n",
      "        [0.2002],\n",
      "        [0.0148],\n",
      "        [0.0756],\n",
      "        [0.0200],\n",
      "        [0.0398],\n",
      "        [0.5878],\n",
      "        [0.2144],\n",
      "        [0.6025],\n",
      "        [0.0578],\n",
      "        [0.0137],\n",
      "        [0.0148],\n",
      "        [0.0150],\n",
      "        [0.0171],\n",
      "        [0.0190],\n",
      "        [0.0149],\n",
      "        [0.0138],\n",
      "        [0.0180],\n",
      "        [0.0240],\n",
      "        [0.0161],\n",
      "        [0.0175]])\n",
      "tensor(0.8324)\n",
      "tensor(0.4371)\n",
      "tensor(0.5132)\n",
      "tensor(0.3277)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<behrt_train.train_behrt at 0x7f96a4190d60>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if data_icu:\n",
    "    token=tokenization.BEHRT_models(data_icu,diag_flag,proc_flag,out_flag,chart_flag,med_flag,False)\n",
    "    tokenized_src, tokenized_age, tokenized_gender, tokenized_ethni, tokenized_ins, tokenized_labels=token.tokenize()\n",
    "else:\n",
    "    token=tokenization.BEHRT_models(data_icu,diag_flag,proc_flag,False,False,med_flag,lab_flag)\n",
    "    tokenized_src, tokenized_age, tokenized_gender, tokenized_ethni, tokenized_ins, tokenized_labels=token.tokenize()\n",
    "    \n",
    "behrt_train.train_behrt(tokenized_src, tokenized_age, tokenized_gender, tokenized_ethni, tokenized_ins, tokenized_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-objective",
   "metadata": {},
   "source": [
    "### EVALUATION AS STANDALONE MODULE\n",
    "Below cell shows an exaple of how evaluation module can be used as a standalone module.\n",
    "\n",
    "evaluation.Loss class can be instantiated and model output and ground truth can be passed to it to obtain results.\n",
    "\n",
    "In the example below we captured model output and ground truth in a file and used that file to read the data.\n",
    "\n",
    "In function definition ***loss(prob,truth,logits,False)***\n",
    "\n",
    "prob -> List of Output predicted probabilities of case being positive\n",
    "\n",
    "truth -> List of ground truth labels\n",
    "\n",
    "logits -> List of logits obtained from last fully connected layer before applying softmax.sigmoid function in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device='cuda:0'\n",
    "#device='cpu'\n",
    "loss=evaluation.Loss(device,acc=True,ppv=True,sensi=True,tnr=True,npv=True,auroc=True,aurocPlot=True,auprc=True,auprcPlot=True,callb=True,callbPlot=True)\n",
    "with open(\"./data/output/outputDict\", 'rb') as fp:\n",
    "    outputDict=pickle.load(fp)\n",
    "prob=list(outputDict['Prob'])\n",
    "truth=list(outputDict['Labels'])\n",
    "logits=list(outputDict['Logits'])\n",
    "#print(torch.tensor(prob))\n",
    "print(\"======= TESTING ========\")\n",
    "loss(prob,truth,logits,train=False,standalone=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-works",
   "metadata": {},
   "source": [
    "### 11. FAIRNESS EVALUATION\n",
    "In train and testing step we save output files in **./data/output/** folder.\n",
    "\n",
    "This file conatins list of demographic variables included in training and testing of the model.\n",
    "\n",
    "It also contains the ground truth labels and predicted probability for each sample.\n",
    "\n",
    "We use the above saved data to perform fairness evaluation of the results obtained from model testing.\n",
    "\n",
    "This module can be used as stand-alone module also.\n",
    "\n",
    "Please create a file that contains predicted probabilites form the last sigmoid layer in column named **Prob** and\n",
    "ground truth labels for each sample in column named **Labels**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-direction",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness.fairness_evaluation(inputFile='outputDict',outputFile='fairnessReport')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-distinction",
   "metadata": {},
   "source": [
    "### 12. MODEL CALLIBRATION\n",
    "\n",
    "Please run below cell if you want to callibrate predicted probabilites of the model on test data.\n",
    "It will use the output saved during the testing of the model.\n",
    "\n",
    "The file is saved in **./data/output/**.\n",
    "\n",
    "This module can be used as stand-alone module also.\n",
    "\n",
    "Please create a file that contain predicted logits form the last fully connected layer in column named **Logits** and <br>ground truth labels for each sample in a column named **Labels**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-flavor",
   "metadata": {},
   "outputs": [],
   "source": [
    "callibrate_output.callibrate(inputFile='outputDict',outputFile='callibratedResults')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-canyon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
