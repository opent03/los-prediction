{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bce4773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/viet/anaconda3/envs/los/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef803ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/viet/anaconda3/envs/los/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "603c8354-fadf-443b-b2d1-ff829cb7df38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viet/anaconda3/envs/los/lib/python3.11/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "\n",
    "module_path='preprocessing/day_intervals_preproc'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "module_path='utils'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "module_path='preprocessing/hosp_module_preproc'\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "module_path='model'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "#print(sys.path)\n",
    "root_dir = os.path.dirname(os.path.abspath('UserInterface.ipynb'))\n",
    "#data_dir = '/datasets/MIMIC-IV/physionet.org/files'\n",
    "data_dir='~/physionet.org/files'\n",
    "import day_intervals_cohort\n",
    "from day_intervals_cohort import *\n",
    "\n",
    "import day_intervals_cohort_v2\n",
    "from day_intervals_cohort_v2 import *\n",
    "\n",
    "import data_generation_icu\n",
    "\n",
    "import data_generation\n",
    "import evaluation\n",
    "\n",
    "import feature_selection_hosp\n",
    "from feature_selection_hosp import *\n",
    "\n",
    "# import train\n",
    "# from train import *\n",
    "\n",
    "\n",
    "import model.ml_models as ml_models\n",
    "from ml_models import *\n",
    "\n",
    "import dl_train\n",
    "from dl_train import *\n",
    "\n",
    "import tokenization\n",
    "from tokenization import *\n",
    "\n",
    "\n",
    "import behrt_train\n",
    "from behrt_train import *\n",
    "\n",
    "import feature_selection_icu\n",
    "from feature_selection_icu import *\n",
    "import fairness\n",
    "import callibrate_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8efe9684-d37f-4a4b-945f-a43e881861fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "importlib.reload(day_intervals_cohort)\n",
    "import day_intervals_cohort\n",
    "from day_intervals_cohort import *\n",
    "\n",
    "importlib.reload(day_intervals_cohort_v2)\n",
    "import day_intervals_cohort_v2\n",
    "from day_intervals_cohort_v2 import *\n",
    "\n",
    "importlib.reload(data_generation_icu)\n",
    "import data_generation_icu\n",
    "importlib.reload(data_generation)\n",
    "import data_generation\n",
    "\n",
    "importlib.reload(feature_selection_hosp)\n",
    "import feature_selection_hosp\n",
    "from feature_selection_hosp import *\n",
    "\n",
    "importlib.reload(feature_selection_icu)\n",
    "import feature_selection_icu\n",
    "from feature_selection_icu import *\n",
    "\n",
    "importlib.reload(tokenization)\n",
    "import tokenization\n",
    "from tokenization import *\n",
    "\n",
    "importlib.reload(ml_models)\n",
    "import ml_models\n",
    "from ml_models import *\n",
    "\n",
    "importlib.reload(dl_train)\n",
    "import dl_train\n",
    "from dl_train import *\n",
    "\n",
    "importlib.reload(behrt_train)\n",
    "import behrt_train\n",
    "from behrt_train import *\n",
    "\n",
    "importlib.reload(fairness)\n",
    "import fairness\n",
    "\n",
    "importlib.reload(callibrate_output)\n",
    "import callibrate_output\n",
    "\n",
    "importlib.reload(evaluation)\n",
    "import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab6d0fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fd7c5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 21 14:47:09 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.113.01             Driver Version: 535.113.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 Ti     Off | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   45C    P8              21W / 265W |    930MiB /  8192MiB |      7%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1552      G   /usr/lib/Xorg                               338MiB |\n",
      "|    0   N/A  N/A      1683      G   /usr/bin/gnome-shell                         74MiB |\n",
      "|    0   N/A  N/A      2126      G   /usr/bin/gnome-calendar                       7MiB |\n",
      "|    0   N/A  N/A      2134      G   /usr/lib/thunderbird/thunderbird              8MiB |\n",
      "|    0   N/A  N/A      2463      G   ...rdCache,DocumentPictureInPictureAPI       65MiB |\n",
      "|    0   N/A  N/A      2535      G   ...,WinRetrieveSuggestionsOnlyOnDemand       97MiB |\n",
      "|    0   N/A  N/A      2723      G   ...,WinRetrieveSuggestionsOnlyOnDemand       60MiB |\n",
      "|    0   N/A  N/A      3322      G   ...sion,SpareRendererForSitePerProcess       60MiB |\n",
      "|    0   N/A  N/A      4080      G   ...40037345,4118063472940996811,262144      180MiB |\n",
      "|    0   N/A  N/A      8124      G   /usr/bin/nautilus                            13MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f457ad-4054-466a-905a-02b3f14b4b0e",
   "metadata": {},
   "source": [
    "### Data is already processed, either stored on Vaughan in ``/datasets/MIMIC-IV/data`` or ``./data``. If on cluster, create a symlink in the main folder pointing to ``/datasets/MIMIC-IV/data``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d673d87-e655-4aa8-bec0-1e6cc4b6f62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_icu=True\n",
    "diag_flag=True\n",
    "out_flag=True\n",
    "chart_flag=True\n",
    "proc_flag=True\n",
    "med_flag=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc8a52a-a8e5-429e-8ca1-f5fd87e02124",
   "metadata": {},
   "source": [
    "### 1. Deep Learning Models\n",
    "\n",
    "- Time-series LSTM and Time-series CNN which will only use time-series events like medications, charts, labs, output events to train model.\n",
    "\n",
    "- Hybrid LSTM and Hybrid CNN will use static data - diagnosis, demographic data aong with other time-series data to train model.\n",
    "\n",
    "- LSTM with Attention model will use attention layer to rank the important features and learn to predict output. It will use both static and time-series data.\n",
    "\n",
    "**Go to ./model/parameter.py and define all variables needed for model building and training**\n",
    "\n",
    "**Please run below cell to select which model to use**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3032234f-a254-4539-b676-fc46d0a070eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "radio_input6=widgets.RadioButtons(options=['Time-series LSTM','Time-series CNN','Hybrid LSTM','Hybrid CNN'],value='Time-series LSTM')\n",
    "display(radio_input6)\n",
    "print(\"Please select below option for cross-validation\")\n",
    "radio_input7 = widgets.RadioButtons(options=['No CV','5-fold CV','10-fold CV'],value='5-fold CV')\n",
    "display(radio_input7)\n",
    "print(\"Do you want to do oversampling for minority calss ?\")\n",
    "radio_input8 = widgets.RadioButtons(options=['True','False'],value='True')\n",
    "display(radio_input8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9055c971",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dlmodels = ['Time-series LSTM','Time-series CNN','Hybrid LSTM','Hybrid CNN']\n",
    "dlmodels = ['Time-series LSTM','Time-series CNN', 'Hybrid LSTM','Hybrid CNN']\n",
    "for mn in dlmodels:\n",
    "    print('Training {}'.format(mn))\n",
    "    model=dl_train.DL_models(data_icu,\n",
    "                             diag_flag,\n",
    "                             proc_flag,\n",
    "                             out_flag,\n",
    "                             chart_flag,\n",
    "                             med_flag,\n",
    "                             lab_flag=False,\n",
    "                             model_type=mn,\n",
    "                             k_fold=0,\n",
    "                             oversampling=True,\n",
    "                             model_name=mn,\n",
    "                             train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e241db5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls saved_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce0bc31-5964-47a8-af88-854f60e883ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if radio_input7.value=='No CV':\n",
    "    cv=0\n",
    "elif radio_input7.value=='5-fold CV':\n",
    "    cv=int(5)\n",
    "elif radio_input7.value=='10-fold CV':\n",
    "    cv=int(10)\n",
    "    \n",
    "if data_icu:\n",
    "    model=dl_train.DL_models(data_icu,diag_flag,proc_flag,out_flag,chart_flag,med_flag,False,radio_input6.value,cv,oversampling=radio_input8.value=='True',model_name='attn_icu_read',train=True)\n",
    "else:\n",
    "    model=dl_train.DL_models(data_icu,diag_flag,proc_flag,False,False,med_flag,lab_flag,radio_input6.value,cv,oversampling=radio_input8.value=='True',model_name='attn_icu_read',train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e053d138-f851-40ed-a7ff-93c47df71409",
   "metadata": {},
   "source": [
    "### 2. BEHRT\n",
    "Below we integrate the implementation of BEHRT in our pipeline.\n",
    "We perform pre-procesing needed to run BEHRT model. https://github.com/deepmedicine/BEHRT\n",
    "\n",
    "Few things to note before running BEHRT -\n",
    "- The numerical values are binned into quantiles.\n",
    "- BEHRT has recommended maximum number of events per sample to be 512. \n",
    "    So feature selection is important so that number of events per sample does not exceed 512.\n",
    "- The model is quite computationally heavy so it requires a GPU.\n",
    "\n",
    "The output files for BEHRT will be saved in ./data/behrt/ folder\n",
    "\n",
    "**Please run below cell to to pre-process and run BEHRT on the selected cohort**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf0b7cb-8dec-4aca-8d74-743ee791cfd1",
   "metadata": {},
   "source": [
    "## NOTE: CANNOT run BEHRT on most local machines. Cluster ONLY. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcdcf2a-8653-490a-91f9-6f66b003b13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_icu:\n",
    "    token=tokenization.BEHRT_models(data_icu,diag_flag,proc_flag,out_flag,chart_flag,med_flag,False)\n",
    "    tokenized_src, tokenized_age, tokenized_gender, tokenized_ethni, tokenized_ins, tokenized_labels=token.tokenize()\n",
    "else:\n",
    "    token=tokenization.BEHRT_models(data_icu,diag_flag,proc_flag,False,False,med_flag,lab_flag)\n",
    "    tokenized_src, tokenized_age, tokenized_gender, tokenized_ethni, tokenized_ins, tokenized_labels=token.tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c12729",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1ba710",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_src = pd.read_csv('data/token/tokenized_src_5000.csv', index_col=0)\n",
    "tokenized_age = pd.read_csv('data/token/tokenized_age_5000.csv', index_col=0)\n",
    "tokenized_gender = pd.read_csv('data/token/tokenized_gender_5000.csv', index_col=0)\n",
    "tokenized_ethni = pd.read_csv('data/token/tokenized_ethni_5000.csv', index_col=0)\n",
    "tokenized_ins = pd.read_csv('data/token/tokenized_ins_5000.csv', index_col=0)\n",
    "tokenized_labels = pd.read_csv('data/token/tokenized_labels_5000.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cd9720",
   "metadata": {},
   "outputs": [],
   "source": [
    "behrt_train.train_behrt(tokenized_src, tokenized_age, tokenized_gender, tokenized_ethni, tokenized_ins, tokenized_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8064ff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.max_memory_allocated()/int(1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf14173",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e85e6c8-2bd3-4094-b23a-2d0c69f3547a",
   "metadata": {},
   "source": [
    "### EVALUATION AS STANDALONE MODULE\n",
    "Below cell shows an exaple of how evaluation module can be used as a standalone module.\n",
    "\n",
    "evaluation.Loss class can be instantiated and model output and ground truth can be passed to it to obtain results.\n",
    "\n",
    "In the example below we captured model output and ground truth in a file and used that file to read the data.\n",
    "\n",
    "In function definition ***loss(prob,truth,logits,False)***\n",
    "\n",
    "prob -> List of Output predicted probabilities of case being positive\n",
    "\n",
    "truth -> List of ground truth labels\n",
    "\n",
    "logits -> List of logits obtained from last fully connected layer before applying softmax.sigmoid function in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2d6786d-5fc4-4538-bd88-9345a265c0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Time-series LSTM\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Loss.__init__() missing 1 required positional argument: 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mn \u001b[38;5;129;01min\u001b[39;00m dlmodels:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvaluating \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(mn))\n\u001b[0;32m----> 5\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[43mevaluation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43macc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mppv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43msensi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mtnr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mnpv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mauroc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43maurocPlot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mauprc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mauprcPlot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mcallb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mcallbPlot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputDicts/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124moutputDict\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(mn), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[1;32m      7\u001b[0m         outputDict\u001b[38;5;241m=\u001b[39mpickle\u001b[38;5;241m.\u001b[39mload(fp)\n",
      "\u001b[0;31mTypeError\u001b[0m: Loss.__init__() missing 1 required positional argument: 'device'"
     ]
    }
   ],
   "source": [
    "dlmodels = ['Time-series LSTM','Time-series CNN', 'Hybrid LSTM','Hybrid CNN']\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "for mn in dlmodels:\n",
    "    print('Evaluating {}'.format(mn))\n",
    "    loss=evaluation.Loss(mn,device,acc=True,ppv=True,sensi=True,tnr=True,npv=True,auroc=True,aurocPlot=True,auprc=True,auprcPlot=True,callb=True,callbPlot=True)\n",
    "    with open(\"outputDicts/{}outputDict\".format(mn), 'rb') as fp:\n",
    "        outputDict=pickle.load(fp)\n",
    "    prob=list(outputDict['Prob'])\n",
    "    truth=list(outputDict['Labels'])\n",
    "    logits=list(outputDict['Logits'])\n",
    "    #print(torch.tensor(prob))\n",
    "    print(\"======= TESTING ========\")\n",
    "    loss(prob,truth,logits,train=False,standalone=True)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fde0a77-beb2-4891-9166-13324562e2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device='cuda:0'\n",
    "#device='cpu'\n",
    "loss=evaluation.Loss(device,acc=True,ppv=True,sensi=True,tnr=True,npv=True,auroc=True,aurocPlot=True,auprc=True,auprcPlot=True,callb=True,callbPlot=True)\n",
    "with open(\"./data/output/outputDict\", 'rb') as fp:\n",
    "    outputDict=pickle.load(fp)\n",
    "prob=list(outputDict['Prob'])\n",
    "truth=list(outputDict['Labels'])\n",
    "logits=list(outputDict['Logits'])\n",
    "#print(torch.tensor(prob))\n",
    "print(\"======= TESTING ========\")\n",
    "loss(prob,truth,logits,train=False,standalone=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ee10d9-304a-4a2c-8944-81ffd3509cb9",
   "metadata": {},
   "source": [
    "### FAIRNESS EVALUATION\n",
    "In train and testing step we save output files in **./data/output/** folder.\n",
    "\n",
    "This file conatins list of demographic variables included in training and testing of the model.\n",
    "\n",
    "It also contains the ground truth labels and predicted probability for each sample.\n",
    "\n",
    "We use the above saved data to perform fairness evaluation of the results obtained from model testing.\n",
    "\n",
    "This module can be used as stand-alone module also.\n",
    "\n",
    "Please create a file that contains predicted probabilites form the last sigmoid layer in column named **Prob** and\n",
    "ground truth labels for each sample in column named **Labels**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758be47e-0b87-4e99-9772-97964698013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness.fairness_evaluation(inputFile='outputDict',outputFile='fairnessReport')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785ac665-33b2-4a51-8917-1836a5ca44e6",
   "metadata": {},
   "source": [
    "### MODEL CALLIBRATION\n",
    "\n",
    "Please run below cell if you want to callibrate predicted probabilites of the model on test data.\n",
    "It will use the output saved during the testing of the model.\n",
    "\n",
    "The file is saved in **./data/output/**.\n",
    "\n",
    "This module can be used as stand-alone module also.\n",
    "\n",
    "Please create a file that contain predicted logits form the last fully connected layer in column named **Logits** and <br>ground truth labels for each sample in a column named **Labels**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f854d5-81dc-47f7-84a4-1ab1149fe70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "callibrate_output.callibrate(inputFile='outputDict',outputFile='callibratedResults')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff3b96d-0bfe-4593-8cb3-d6fc3b5b7986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
